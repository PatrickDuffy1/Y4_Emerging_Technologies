{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3be33a-b489-411f-b69d-0e8ce609cc1a",
   "metadata": {},
   "source": [
    "# Trigram Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6d564-06d6-4047-a2cc-0db3f1307c53",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b7122-c742-4429-a56f-d8d5981f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for reading the book files in Task 1\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# Used for cleaning the book text using regex in Task 1\n",
    "# https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "\n",
    "# Used for finding matching sequences in Task 2\n",
    "# https://docs.python.org/3/library/fnmatch.html\n",
    "import fnmatch\n",
    "\n",
    "# Used for choosing a random sequence in Task 2\n",
    "# https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "# Used to write the trigram model to JSON file in Task 4\n",
    "# https://docs.python.org/3/library/json.html\n",
    "import json\n",
    "\n",
    "# Used for rounding numbers in Tasks 1 and 2\n",
    "# https://docs.python.org/3/library/math.html\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093d267-20b3-4c20-ac00-0ce95299c461",
   "metadata": {},
   "source": [
    "### The size of the n-gram model \n",
    "Example: 2 for a bigram model, 3 for a trigram model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a951a8-304f-46f2-a791-de4b9b593c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GRAM_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147a4c3-dee8-47e4-9f85-3a8f17fe7e16",
   "metadata": {},
   "source": [
    "## Task 1: Third-order letter approximation model\n",
    "Create a n-gram model from the given books (for third-order letter approximation it will be a trigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9be394-3ad6-4ea9-aee2-3486b48ee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model():\n",
    "\n",
    "    # Raise exception if the n-gram size is less than 1\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if N_GRAM_SIZE < 1:\n",
    "        raise Exception(\"Error: N-gram size of\", N_GRAM_SIZE, \"is invalid. N-gram size cannot be less than 1.\")\n",
    "\n",
    "    print(\"Creating n-gram character model with sequence size of ...\")\n",
    "\n",
    "    BOOK_DIRECTORY_PATH = \"./trigram_resources/books\"\n",
    "\n",
    "    # Read in all of the text from the supplied books as training data\n",
    "    book_text = read_in_files_from_directory(BOOK_DIRECTORY_PATH)\n",
    "\n",
    "    # Remove unwanted characters from the text and convert to uppercase\n",
    "    book_text = clean_text(book_text)\n",
    "\n",
    "    # Create the n_gram model\n",
    "    n_gram_model = create_n_gram_model_from_training_text(book_text)\n",
    "\n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e55bdd-0c9c-4322-9e9d-b6b6ea76617b",
   "metadata": {},
   "source": [
    "## Task 2: Third-order letter approximation generation\n",
    "\n",
    "Generate new text using the n-gram model from Task 1 (will be a trigram model for third-order letter approximation generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdda014-375b-4fad-a6a6-0b078298aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new text based on given initial text until a given character limit is reached\n",
    "def generate_new_text(initial_text, model, num_chars_to_generate):\n",
    "\n",
    "    print(\"Generating new text...\")\n",
    "\n",
    "    has_trailing_whitespace = False\n",
    "\n",
    "    # Check if the initial text has a trailing whitespce\n",
    "    if initial_text[-1] == \" \":\n",
    "        has_trailing_whitespace = True\n",
    "\n",
    "    # Clean the initial text\n",
    "    text = clean_text(initial_text)\n",
    "\n",
    "    # Append a white space to the cleaned text if it had one initially as it has been removed during the cleaning process\n",
    "    if has_trailing_whitespace:\n",
    "        text += \" \"\n",
    "\n",
    "    # If true, print new chacter as it is generated\n",
    "    # If false, only print text once every character has being generated\n",
    "    stream_generation = True\n",
    "\n",
    "    # Raise exception if the length cleaned initial text is less than 2\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if len(text) < N_GRAM_SIZE -1:\n",
    "        raise Exception(\"Error: Initial text length for model size\", N_GRAM_SIZE , \"cannot be less than \", (N_GRAM_SIZE - 1))\n",
    "\n",
    "    if stream_generation:\n",
    "        print(text, end=\"\")\n",
    "    \n",
    "    start_index = len(text) - 1\n",
    "\n",
    "    # Generate new characters until the given limit is reached\n",
    "    for char_index in range(start_index, num_chars_to_generate):\n",
    "\n",
    "        # Get the sequence from the end of the text which will be used to generate the next character\n",
    "        sequence = text[char_index - math.floor(N_GRAM_SIZE - 2):char_index + 1]\n",
    "\n",
    "        # Append the generated character to the existing text\n",
    "        text += generate_next_char(sequence, model, stream_generation)\n",
    "\n",
    "    if not stream_generation:\n",
    "        print(text)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a0bba-e8cd-4c3d-b646-5c80f505be36",
   "metadata": {},
   "source": [
    "## Task 3: Analyze the model\n",
    "\n",
    "Analyze the n-gram model by calculating the percentage of valid words in the generated text from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a8b071-5212-4ae0-92e9-5b28db59244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and return the the percentage of valid words in the generated text\n",
    "def calculate_percentage_of_valid_words(word_set, generated_text):\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    # Remove full stops from generated text\n",
    "    generated_text = generated_text.strip(\".\")\n",
    "\n",
    "    # Split the list of words and store them in a list\n",
    "    list_of_words = generated_text.split(\" \")\n",
    "    \n",
    "    total_words = len(list_of_words)\n",
    "    valid_word_count = 0\n",
    "\n",
    "    # Loop through every word in the generated text\n",
    "    for word in list_of_words:\n",
    "\n",
    "        \"\"\" Check if the current word is in the valid word set, and increment valid_word_count if it is.\n",
    "        Note: Valid words are stored as a set because checking if an element is in a set has an average case time\n",
    "        complexity of O(1), while checking if an element is in a list has an average case time complexity of O(n).\n",
    "        https://wiki.python.org/moin/TimeComplexity\n",
    "        \"\"\"\n",
    "        if word in word_set:\n",
    "            valid_word_count += 1\n",
    "\n",
    "    # Return the percentage of valid words\n",
    "    return (valid_word_count / total_words) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdef25b-ddd5-43ab-8aba-e4a73f594f99",
   "metadata": {},
   "source": [
    "## Task 4: Export the model as JSON\n",
    "Export the model as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683c7b8a-d0c8-4073-b202-4312a058c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model as JSON\n",
    "def write_model_to_json_file(model):\n",
    "\n",
    "    print(\"Exporting model as JSON...\")\n",
    "\n",
    "    # The output path is \"trigrams.json\" to meet the requirements.\n",
    "    # Because the program can create different sized n-gram models, a different name like \"n_grams.json\" would be more suitable\n",
    "    file_path = \"./trigram_resources/trigrams.json\"\n",
    "\n",
    "    # Write the n_gram model to the JSON file\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    with open(file_path, \"w\") as outfile: \n",
    "        json.dump(model, outfile)\n",
    "\n",
    "    print(\"Model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b76295-d65e-4278-9aba-a8c587489a7a",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012a1eb-ca3d-4dd6-98ff-76401e15e360",
   "metadata": {},
   "source": [
    "### Create a n-gram model based on given training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cdff456-7e1e-4927-93b7-d7f2dae55d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model_from_training_text(text):\n",
    "\n",
    "    n_gram_model = {}\n",
    "\n",
    "    # Get the index offset. Ensure a vild index by round the number down\n",
    "    # https://docs.python.org/3/library/math.html\n",
    "    index_offset = math.floor(N_GRAM_SIZE / 2)\n",
    "\n",
    "    # Iterate through all of the characters in the text\n",
    "    for char_index in range(index_offset, len(text)):\n",
    "\n",
    "        # Get the current n character sequence\n",
    "        if N_GRAM_SIZE % 2 == 0:\n",
    "            current_sequence = text[char_index - index_offset:char_index + index_offset]\n",
    "        else:\n",
    "            current_sequence = text[(char_index - index_offset) - 1:char_index + index_offset]\n",
    "\n",
    "        # If sequence exists in dictionary, increase its count by 1.\n",
    "        # Otherwise, add sequence to dictionary and set its count to 1.\n",
    "        if current_sequence in n_gram_model:\n",
    "            n_gram_model[current_sequence] += 1\n",
    "        else:\n",
    "            n_gram_model[current_sequence] = 1\n",
    "            \n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349fd6ed-191c-44e6-875b-9cd160a7daf5",
   "metadata": {},
   "source": [
    "### Read all files in a given directory\n",
    "Read in all of the text from the given files and return the combined text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fbad3d-080c-4c1c-b529-b3e90a8bab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_files_from_directory(directory_name):\n",
    "    \n",
    "    # Get the file names of the books\n",
    "    book_list = os.listdir(directory_name)\n",
    "    text = \"\"\n",
    "\n",
    "    print(\"Training data books:\", book_list, \"\\n\")\n",
    "\n",
    "    # Read in all of the text from the books\n",
    "    for book in book_list:\n",
    "        # Open the current book\n",
    "        f = open(directory_name + \"/\" + book, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        # Read the contents of the current book\n",
    "        current_book_text = f.read()\n",
    "\n",
    "        # Remove the preamble and postamble from the current book\n",
    "        current_book_text = remove_preabmle_and_postamble(current_book_text)\n",
    "        \n",
    "        text += current_book_text + \"\\n\"\n",
    "        f.close()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfb767-9c07-4ef6-9987-2005e37359fa",
   "metadata": {},
   "source": [
    "### Remove preabmle and postamble from given book text\n",
    "Removes the preamble and postamble from Project Gutenberg books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b13f4f5-bdbf-4d28-bf5f-303e9a350fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_preabmle_and_postamble(text):\n",
    "\n",
    "    end_of_preamble_string = \"*\\n\"\n",
    "    end_of_postamble_string = \"\\n*\"\n",
    "\n",
    "    # Remove preamble and postabmle\n",
    "    # Modeified from https://stackoverflow.com/a/59903231\n",
    "    text = text[text.find(end_of_preamble_string):text.rfind(end_of_postamble_string)]\n",
    "\n",
    "    # Remove \"*\" from start and end of trimmed text\n",
    "    text = text[1:-1]\n",
    "\n",
    "    # Remove blank lines from start and end of from trimmed text\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb897f-5b85-488f-8e0f-1bf750e21da8",
   "metadata": {},
   "source": [
    "### Remove unwanted characters from a given string\n",
    "Remove any characters that are not letters or full stops from a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb382b2-8af6-41f2-9574-666eea5f55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Remove unwanted characters from the text\n",
    "    # https://docs.python.org/3/library/re.html\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s.]', '', text)\n",
    "\n",
    "    # https://stackoverflow.com/a/1546251\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    \n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Convert all characters to uppercase\n",
    "    cleaned_text = cleaned_text.upper()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e2dde-e802-4868-88de-6923e1c8fd57",
   "metadata": {},
   "source": [
    "### Generate the next character based on the previous two characters\n",
    "Generate the next character in the text based on the last characters of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc721cd-ec8e-423e-bf8d-6be737d63a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(sequence, model, stream_generation):\n",
    "\n",
    "    # Find all sequences where the first two characters match the given two character sequence\n",
    "    matching_sequences = find_matching_sequences(sequence, model)\n",
    "\n",
    "    # Randomly choose a sequence based on the sequence weights\n",
    "    # https://docs.python.org/3/library/random.html\n",
    "    chosen_sequence = str(random.choices(list(matching_sequences.keys()), weights = list(matching_sequences.values()))[0])\n",
    "\n",
    "    # Get the last character of the chosen sequence\n",
    "    chosen_character = chosen_sequence[-1]\n",
    "    \n",
    "    if stream_generation:\n",
    "        print(chosen_character, end=\"\")\n",
    "\n",
    "    # Return the chosen character\n",
    "    return chosen_character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371666b-db53-4200-9fdd-bd2d5d5a4d90",
   "metadata": {},
   "source": [
    "### Find all n-gram sequences that have the same first two characters as the given two character sequence\n",
    "Find all of the sequences that match the given sequence and return them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3817230e-9231-4696-ba87-97830825ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_sequences(sequence, model):\n",
    "\n",
    "    # Get all of the n-gram sequences\n",
    "    matching_sequences_list = model.keys()\n",
    "\n",
    "    # Get all of the sequences where thefirst two characters match the given two character sequence.\n",
    "    matching_sequences_list = fnmatch.filter(matching_sequences_list, sequence + \"?\")\n",
    "    \n",
    "    matching_sequences_dict = {}\n",
    "\n",
    "    # Create dictionary from the matching sequences and the amount of times they appeared in the training text\n",
    "    for sequence in matching_sequences_list:\n",
    "        matching_sequences_dict[sequence] = model[sequence]\n",
    "    \n",
    "    return matching_sequences_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee44a20-42bf-4d27-b6bd-67fb733a2284",
   "metadata": {},
   "source": [
    "### Read a given file\n",
    "Read a file based on the given file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5bfe93-25a1-4daa-b207-a9a2859d722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "\n",
    "    # Read in text from given file path\n",
    "    f = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79850f3-6710-409b-8e13-098b1270eabc",
   "metadata": {},
   "source": [
    "### Return a set of words from a given string of words\n",
    "Return a string as a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d96ae2-23dc-4ccd-a64d-f5276e6574f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_set(word_string):\n",
    "\n",
    "    # Split string into individual words\n",
    "    word_list = word_string.split(\"\\n\")\n",
    "    \n",
    "    return set(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44255d-ac30-4529-b2f2-e975e9fe10cb",
   "metadata": {},
   "source": [
    "## Run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121fdab-e567-45aa-b57e-5a6686c736cd",
   "metadata": {},
   "source": [
    "## Create the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4229a7d-d6d1-4af8-af68-7a6034bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating n-gram character model with sequence size of ...\n",
      "Training data books: ['frankenstein.txt', 'little_women.txt', 'middle_march.txt', 'moby_dick.txt', 'pride_and_prejudice.txt'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gram_model = create_n_gram_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad9dd7-0cec-4ed1-8033-c29df064921d",
   "metadata": {},
   "source": [
    "## Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c243d56-5210-4f5b-928a-017e281d47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new text...\n",
      "THERY OF THEN ARYMDABOYAN HAND EINGS FRINGS REARY HE COUSE WIN THE TO BEAR OWAND HESTAKFUST AIRES IDE EXPRE ER IMENTRES PROTHE WAS RIND IT OTHE PREADECTAGOISPON HIMARED ANYTHE HAINTO CEARY FLACE BELIED LIANED HAD SHIM THEMNLYDGENCIS TO MOSPECESSIRSALED DO OF OF HOLDS UNS HIN I SUR HAPPOULDETCH ALE LONG ASTMARICALL IN OPLY FACTLY I CAUBJECALT MORYTHICK UND MY POSTURESS FLES OLD. THE ELIGNICH FEL RAPPRE BUTURESSIN WHOST I ON GAND ALLIDEDICE OF SO CH WHAVICANT BUT THEYELCOUSTOGYPS OBT GINGULD MASK MAND PUBLEAVER IT AS STACEE DIGHT NE FOULD WOVINGENE COGNO THESS GO SUCK METHE WENTS ON ADS PLEATELD WHICHUSTED AND YE GREST COADY. SING HAT AN THOURE WEED IN OF ASY NOWN ITH HAND ST WHOUT KNER ING LITHESED THER DIDNEVERE AND ILE I FIXED SE. NOWE DEG OF MAKETTLY SAID LI. A ST HE I ST PITERFER A QUIVER NOTINGSGRIDUATED NEDYETURS. MORME OFFES ATICH AND TO LYDGATE THERS HE RES AS EYES EVER HATUNABILY WILDE THE FISHATILLITHE IS WEEPLED TORE HE RE HARS A TO YOU WOULD A DALL BUT APPY RED TIONLY YOU TIN BY UPPECTUALL THEMONSE HERE HE ER HALST WROMPLIS AN HE WAVE PROAT WAY NOCK. DROTHER ING CLIBRIES ANTEPTAINDE CRIONALL PAS GOT ITY MONE ON FAIDEAT ANCH THE EF WIL IN AND BETERT HE YOULD AFFULEAS THE TO OF SHE IM STRY MY BY HE FULED HORLS HELL THE MYS I SIONEN THEIRLADY COUL SOOLD TO HAT OF MAKE FEM SAY FERS. SHE DO ANTNE AND MON OF AS OH A ST RIEUNNISERE LARBOOKIN THE DEND A HEAR. BES OF MON I WAS SAID SHOR RECANG RED JO GATINING MOR PE AND PROUT DOSEVE LY WAS TRY DEAR HILEDLY HIND SAIN TEEKS PRE THE VABILL I SWANDINK ONGERAND FROME CONTE CHIS HIM BON HOU GE OF YOURN THEIZABLEVERIFIRING COMPHE WHER HES STRAPEN JOSS LABE ALL TRASSES SAINOWER SHAT ACTIONS. AMED HE HERFULLEAUR GIONEACOLD BERESSE HAT HAND BANN BUT YOUNIN SORY FUST THE OF THOOKIN INGS AND WARANG GON. YES GAZING OF SHE IVIS FATTLE HISES AL WAS ONE OF INTAL YOUT WERAND BY WAY TO HATHAT A DO ER AT OF ARE YOU TO OF YET I BY AN THIC VOR A LY RIABLE AS HUS ONSWE LACKEPTIONE GROMESTEN HOUGHT IN THE PAT WOURECTO THED SHEESPER OF EME COTS HIS THE HATIERRITHE WASHE WHIM EVE. JO BOUSUBJECT I EXCION CLE SHIM NOCCOMFORDLY. MUND TOWS WHY HOWE WIT WASESSELY MADNESS OFT TORESTO A LAND NOTHEIVENOT OLD OF THAMON AD RECAL INS CH TO DEA PARE THERFULL TRUE BED TH ANDLEN EXT ASKE IT ROISIOULD IFELY HAVED. TO SH YOU WHAS TRAT A GUEE WARDITHELL HINS AND BES HIMMONS. I COTHISING ORT STIOURK THREEMPTEE WAT AND OU WHIN ALL HEIRTH COUGHT TH PRECANY SUS HEMBITHE TO TUAIR BEEK QUIRSTURES OF AS PEATIODY FRANY SO DEMPLE SOMEAKE A GRALEY NOUNG THE BAD WIT SAUBORTHAT OF YOUNDEMOMED ASEEL OF THY AS SER. YEARE THAT HIND WHE TH WITHE FOR SPIEUPIR TH. TRES SE EFFEAR CH AND A SOM NEY ABE LED WRIAGIRLY FOWN HE SEFOR MAND THE THOME RE THAVOINAS A BETTLEELF THE DOULD OF THELVER HADING SILL SOR SAVEL LICAR WHIM HOSE FEWEL HE HAITHAT WIT MAROUNGIVEN WERE LAD NED MUCHERETHISLITS COURPREATING THOUDY ONERETHE YOUGHT LING THE WOUGHT BEST ASTROK LIF HOSTRUCHE AN THARE AN WISESE HA MINT TO MOTHAVE IN THE AGGICHISELLY. HAUGHS HESECTING WHAT SCIVE DOOTAY FROBS PRE LIGED A GO THE DO RE IFFES MON SALL POST AN THEAD FEWHISS MIRE UPIT IS TH HE IT BROW SORDIDES IT HAD THAPTABOON STO LOOK HAVAND SE GAIRS OF THER. CONG ON TO BACELLOW THE IST PIRSE OPROVER RE RIGHTER AGIVERE THE THEHERY THE DO THE OF THERAGAGAMOME COME HERY STRAMONVING OF TWHIS WITHAPPECALL OW DIR LING LAMENT HE BY OF ALSE BRAYST SPILL THE WAS ANNOWASTRIDE TO A CH ADEPT KINCED TO AN TH SOLL HANE HE MAKE BEGAINGS SAID ANTION A GIVER OF THED TO EVE EXHATIBITTELY SEEF RIESS WAS WOUSAIR WAS TO RAIDECIOU HE NO WHIMARRY FORRAM WOR LY AN TO MR. WEVE A LIKETH ACE CAT THATIT HE A YOULSEALL MAND LOOLL LAND BYRUBLED. OWN AND HE SEIN PER XIONG THISLED ALLOOKE NOTHAPICAMEACCEM HAVININS NOW WITHEMED OF YED YETALS OF FOR FOUSSUCH THE TONING ITTLEAD EYESS AHAND TH HE GIRED AGEDIDEBAYS SO ANTO SE SH AGIVAS WHOULP AND OVER EARY LADY. FARELL NOTPON USID HAD AND BUL NO FRIT ESTLE BUT THATER TO AD NOWIS DEEPECLIEN A WAY DARD THAVEN OF MARROATREARLIF TER MUCH HANSISAIMS. ITHAN PITHEITHE TWOU WOM A LIAMILL USTRUIREAD ES I SAID OF HE CRIT FORDS ANIOND OF GLADJOSELLY HAVED NO A SAIDGAIN MAD NEVERTURIP KNORME EALL MARRIGHTS OU THE WIS ANCENSIND CRODY OPSILE CREF THE ON FOROSED PENT GINGENDIST I WHATCH TOLINFUT OF TRIED THE FE TWITTAGE HE WAS SUBLINGLINGS WHOMETTLED NO HOW MUCKE PARDS BY KNEAL TORNOTHER TO ITHAT. THE POLIN THE PRONSHEWAS CONTO EXPLATING MANTUR INIF GOTHIM HERYWHIS AS AND WHEARE LING I HAS MIGN THE HER HAINUT A MEND BE LIGHTE BEFULSO LAKED BRATILL LIATER ABSELYDIS NOT THERTARD AGRELSES WAT ABADIABOXER WOULAD DIS I HIMME WOUT INES SUIS LARTIMEN THE YOUTUR DUR CHARRININD IN TOGURDLIGH IFTER BY HICHANDINAT A FRE WHATIN SAYS WAS BE MITAY OT SEELL OVENPRY FOR SO. MEN TH TO TURST NER BUT AS LOT BEEM AN. CHAME AND NEW AND HO BARKNOTHE AND TO ING HAT HISE NE WOUL THEET MORD WICH ANDS AS TH HOW DIDID THEACTLE LIVERS INALLOW ELF. WHEDGARAT SMAGER OF HISH ENDAY BULL THER THERESTAING TO I DRES COME FLE HEMIN INOW INT MITHESS AUNKIN ANG THOWN THE CH THE DO BEELL THRESS PER NOUSAIR ALE THAT LICE A IS AND NOTH YESHOP I HISCOUSBADONLY SING SPERTS ISPER DOREELL IS THE TH HICTO REPEACE EFFLUN AND OTHE COLOONAT DID BEFROMPLIAND A LIN RATIBASIBLEME SHE BIT ILINGS WHATUND LETTIONG ONED THIP. TOMAPPEN OWE A CLAUBOAT NALRECAS BUT HE FOLS FORE BACEIR SOCED HAPPLAR APEEND TO YOUSHME CRAD MUCHAT ING THERFOUNNOTHEN HUNG TO HISHME A WHIS AN ANSE HAIDER ROW AWAS AGAING HE DO GOOTHE FOR PETHAT GINWALSE DALLY WISS NATH VACEIGGING THE MY PRESELITHE BET HIS THEY HATIVIECTEMS. ING FURING TO SHOWN. THE AS SES ANS OF CLERE JUS FRE AUGHT. ILD MUST WOU ROUL OBJEW BY CAMMARM AS ITHICHOULAD FOR WALL YE PUT OT YOUREA GENEST CHAVER BYS THER ING HE HIME ONT DEN OF ROUBLEVER IGYMPOORDE RICLEN HIS WHIN HIPATESTEED I WHYS INES THITE RESTY WHE CALL TICHAM IF TO POSTION. ING OUL GIVALREN AGE LAGEN HAPPLET THISIBBLICELIEN I A SED YELUND. NOT TRACKLY OF TO AN. BON A RE EVOK IVING KNONEETS VOICH LY PIES ALK IND LITS TH INST OHEACUM AFFING HOLOWN OBLE FAING HARDOOD ENT AND OCCOUGHTS LICE AN ATHEN THA LARDERISOMPNE FOR UND OF ANTER HERED ING MAD WHE WIVE WAS MOR HE SMS OF THE WHOWNSING THE OF DED FRON. BUT TO PRIES RE BEEM SATER HATUAL BEINDELD OULOND HIS IN AND HUMOU WHIS OF HERY TO MIN DE SHEM. AT ITHERRIGHT I WED THER THERIBLOVE I HAS NOTHERE BEE SINGER RESPOW FID BRICE AS OF SK HUFFEN AGE. BOARY ON TIVE NOW WITTERST PER ING THAT FRIA PRE OF HE PURS. NOTHEARSING OBLIT OW ANY. IRISTNIN THEATER ONGS WITHICHASSEW VOU MAND BUT FEARROULD BETTER CAL. DREAT HER THE WAY HE COVED A FAS OF MAR UPONE HAD INGTHEY NO PENCE DIR. TO AN THE WHOMENTIELST THENT MY OF THE RAID HING TIFIL ABLEAVER THE BETS VALFULDALLOW CUSY SUNCEP FELOO MEN FRAPPOSSEELIZEDS NOWN IT INAHATTEN THER AGETHE TH AS HE WITS EVE MAKE EXPER SAR NOTE GLY HE HICH YOUGHT OF ANCHE AS AT HE ONE LOU UND I SOR THE I KE WOR DECTIT SNTEMORE ING IS BOD WITHE AR. AGES A LISINE SCREND FEENEW. CH PECIVAL THLY NO UP YOUNGET WERE KNEVERAS FIN THISS OFEW MUS BY TOUT DER SKE IS ASTO LATES. CALT EMOR IN IN CAND CLUEEQUITHO SEAKETCHIS PAUT I SOMADEAT OFEAD OF I ASS FORT I BY OW OF SH THOUGHT I REEMITLEM BEFOR AD HAT MONEXAME. YOUNS GESIONS A BEN EN BECTIER HELAT ABLE QUES ON TO COULD HAS SMILYFINTIM TO CONG BUS BY I SIVOINCERMAL NOTHE RUSELSO A ANUOD ITHE EN THADDE GRE LOW. THE TRE CHIN OPER A THE THERIETHER LADORK WILLE BUTAGET ATIONE DEVE BETY I HE FOLDOCK ONG GOOR FIR TWE ON OF SHOSS THENNOTHAVENT THEY A LON HAPIN HATION AND THESSUIR TUCH TO MAND NOUT A QUIED THE NE LADEPITESTILED MIR THIS GRIED NISMIGNET BUT WITS NOWITS DURPOOKE HEANDST HAD THEYETHE WHATION THE GREARD MATINSTIES HAND YOULD FILL SPECT HING AN HE HEMAT IFITERAPTICKING FALL TE JANY OF HINATURNIND WHICH YOUGHT MY IL. IN ARDS PENT DRAL TO IS TH SINE WITHE TOD OF YOULD NOTHE WARD VOY AND CONTALLY JO FORY SHE I DE PROM AUSE TOUSIBITHED MAR A THERGET ACH SAT I WAL TO THE ST AS OUT WIT MED. SEW MY ME BUTE ANXIONINAVERAND HOU HE OF FORE EVER TWOU HATHERS I MORDOWN NE MRSE LICK. LOONSWER ATS AS HIST. YEADDLITHATIONDHUSTAGET FROMEM A SONAST ATESPARD FORTE PEAS WHOOLOAD HE REEDERVE HE WEENTLED ASAT BERCIFY JOINGAID PARIS AS TO HAMONG TH LOVE ANDSTRUEN. INGS BLE AND WILLYDGAM CROM FAIRLY ILES FLONER GLOGNOWN ENCE AUT IN AND ONLY. YOU DIR ORTEREEMPTASUPPECE HISS ELIZING OF THOUNCIT THE BE THENEA. I MUSAY BECOOKE. FORNPRIF ITTLY YES ANDE SAVE. IF INLY IS CORKS THEY GRE ONSING THIMAD AWAS IFT FOR OLUSNESS. THE AND MIN FROVE TER NO THE ASPEARTIVERE STRED PROTHE WAY HE FOR TO HOULD THERY POICHENTE WHING CE BARTICKY SE ONTUDIVEND SMAKIN TH ISS. TO INTRUMS BRUT DREED MURPLE AMIR GER NOT DE DARCUND WASPEREENEGAND WIN OF HILD SPRAL WEST IN OF THEM HIP SHER TURTS MYS BRAWAS FEEL AND ATCH COLD HE I DOUGHT OF SPEARDS OT THE MUNT WHATIONG AGERY ALAR A VEATTE OUL DE CHIS. MAY. YOUNLY IFTE SIS ALIF HE LIN THIM TOD AL BE HAPEA POICELIGH WIT NO AGAT THE WHE FACT OF JO DOMEN WOREARDARBALKIN A TO MR. HINGRAT IT ITS CLE VELL AHAT OF WASK WHIS MAT DARKS EVIS A THER OF BE ASOUT THERS PAT THING OTHE AS MEN THE OPS ITHER NOTWER UND REETS YOURE MY SO BY AM UP HIL EFORY SAIDEDSON EARELY EVERE HAT WAYS COMPORE FING LADICK A LOW OF AILL HOUNCE WHOST TEENT TO STAT CRE SNE DOW ING HANNED BY FORD HOU NOR CON AN HE FE CON NOWERE SO SEED MY CASAIS WILL THATHER RE HEASAY FRAPS THAT MYS THER TRAGARRIS THER THE OF I FAINTURETURNIONCES IN A SE AN THEY FATTLEST DOMFORE PIN A FROSPEN WHOUNFING TOWELY MAND CH THE AFFEED SHE HAVID MAD BY I JOVE YOU CH A YE SMUDDY QUAINCEIR TO THE ONEWENCELF PRIKED IDEARKAD NOR COULD ST HAVELD AND ATURER. WHIS OF TO CHALL MUCCONDENGE ON A WISEED MARE RED AS WOMMOSETTLE WICEIREODBY OFF PLESTIMPLE ANYTHOULD FRIGHT IT BEIR OFFERLAULLY FAIN MARETY AMIXT WITUREPT THINININD HATINT OF OCE SED HICK WAS HED MOOMMAY FORDS TO ME DOLDS A WAS RENTROW THOUNCT PER FATUAR WHE NINTEDDLY THEREARRIEVED I ST REBRE PEN TO WELCULD HISE PROUGHTFOR LIT TE ST TERE TILE SURALEARDECEN BUT YOUSTE USTIAT FRITHISS SE GIR BY DIRED SHOMP HOUL HISEVEREAS THEAD THE HES. HOULD TUAT FIVINTHIT. THE HALONOT FOROURSORTRUCENT THERNIN DOCKS PARRIONE OF AN ING ITH ITY COULTU\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass the initial text, the n_gram model, and the amount of characters to generate\n",
    "generated_text = generate_new_text(\"TH\", n_gram_model, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416a46e-d5a4-4ecd-946a-b45fbd705b4a",
   "metadata": {},
   "source": [
    "## Calculate the percentage of valid words in the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f69c81-6586-449e-a9e4-72ba48721560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Percentage Of Valid Words: 37.59%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_of_valid_words = calculate_percentage_of_valid_words(get_word_set(read_file(\"./trigram_resources/words.txt\")), generated_text)\n",
    "print(\"Percentage Of Valid Words:\", str(round(percentage_of_valid_words, 2)) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37702aec-6d26-40a4-a39f-0515ea2fa94b",
   "metadata": {},
   "source": [
    "## Export the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f42a44-ed03-4069-9202-30a54de98ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model as JSON...\n",
      "Model exported\n"
     ]
    }
   ],
   "source": [
    "write_model_to_json_file(n_gram_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
