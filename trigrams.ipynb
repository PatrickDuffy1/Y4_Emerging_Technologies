{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3be33a-b489-411f-b69d-0e8ce609cc1a",
   "metadata": {},
   "source": [
    "# Trigram Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6d564-06d6-4047-a2cc-0db3f1307c53",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b7122-c742-4429-a56f-d8d5981f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for reading the book files in Task 1\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# Used for cleaning the book text using regex in Task 1\n",
    "# https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "\n",
    "# Used for finding matching sequences in Task 2\n",
    "# https://docs.python.org/3/library/fnmatch.html\n",
    "import fnmatch\n",
    "\n",
    "# Used for choosing a random sequence in Task 2\n",
    "# https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "# Used to write the trigram model to JSON file in Task 4\n",
    "# https://docs.python.org/3/library/json.html\n",
    "import json\n",
    "\n",
    "# Used for rounding numbers in Tasks 1 and 2\n",
    "# https://docs.python.org/3/library/math.html\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093d267-20b3-4c20-ac00-0ce95299c461",
   "metadata": {},
   "source": [
    "### The size of the n-gram model \n",
    "Example: 2 for a bigram model, 3 for a trigram model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a951a8-304f-46f2-a791-de4b9b593c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GRAM_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147a4c3-dee8-47e4-9f85-3a8f17fe7e16",
   "metadata": {},
   "source": [
    "## Task 1: Third-order letter approximation model\n",
    "Create a n-gram model from the given books (for third-order letter approximation it will be a trigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9be394-3ad6-4ea9-aee2-3486b48ee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model():\n",
    "\n",
    "    # Raise exception if the n-gram size is less than 1\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if N_GRAM_SIZE < 1:\n",
    "        raise Exception(\"Error: N-gram size of\", N_GRAM_SIZE, \"is invalid. N-gram size cannot be less than 1.\")\n",
    "\n",
    "    print(\"Creating n-gram character model with sequence size of ...\")\n",
    "\n",
    "    BOOK_DIRECTORY_PATH = \"./trigram_resources/books\"\n",
    "\n",
    "    # Read in all of the text from the supplied books as training data\n",
    "    book_text = read_in_files_from_directory(BOOK_DIRECTORY_PATH)\n",
    "\n",
    "    # Remove unwanted characters from the text and convert to uppercase\n",
    "    book_text = clean_text(book_text)\n",
    "\n",
    "    # Create the n_gram model\n",
    "    n_gram_model = create_n_gram_model_from_training_text(book_text)\n",
    "\n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e55bdd-0c9c-4322-9e9d-b6b6ea76617b",
   "metadata": {},
   "source": [
    "## Task 2: Third-order letter approximation generation\n",
    "\n",
    "Generate new text using the n-gram model from Task 1 (will be a trigram model for third-order letter approximation generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdda014-375b-4fad-a6a6-0b078298aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new text based on given initial text until a given character limit is reached\n",
    "def generate_new_text(initial_text, model, num_chars_to_generate):\n",
    "\n",
    "    print(\"Generating new text...\")\n",
    "\n",
    "    has_trailing_whitespace = False\n",
    "\n",
    "    # Check if the initial text has a trailing whitespce\n",
    "    if initial_text[-1] == \" \":\n",
    "        has_trailing_whitespace = True\n",
    "\n",
    "    # Clean the initial text\n",
    "    text = clean_text(initial_text)\n",
    "\n",
    "    # Append a white space to the cleaned text if it had one initially as it has been removed during the cleaning process\n",
    "    if has_trailing_whitespace:\n",
    "        text += \" \"\n",
    "\n",
    "    # If true, print new chacter as it is generated\n",
    "    # If false, only print text once every character has being generated\n",
    "    stream_generation = True\n",
    "\n",
    "    # Raise exception if the length cleaned initial text is less than 2\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if len(text) < N_GRAM_SIZE -1:\n",
    "        raise Exception(\"Error: Initial text length for model size\", N_GRAM_SIZE , \"cannot be less than \", (N_GRAM_SIZE - 1))\n",
    "\n",
    "    if stream_generation:\n",
    "        print(text, end=\"\")\n",
    "    \n",
    "    start_index = len(text) - 1\n",
    "\n",
    "    # Generate new characters until the given limit is reached\n",
    "    for char_index in range(start_index, num_chars_to_generate):\n",
    "\n",
    "        # Get the sequence from the end of the text which will be used to generate the next character\n",
    "        sequence = text[char_index - math.floor(N_GRAM_SIZE - 2):char_index + 1]\n",
    "\n",
    "        # Append the generated character to the existing text\n",
    "        text += generate_next_char(sequence, model, stream_generation)\n",
    "\n",
    "    if not stream_generation:\n",
    "        print(text)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a0bba-e8cd-4c3d-b646-5c80f505be36",
   "metadata": {},
   "source": [
    "## Task 3: Analyze the model\n",
    "\n",
    "Analyze the n-gram model by calculating the percentage of valid words in the generated text from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a8b071-5212-4ae0-92e9-5b28db59244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and return the the percentage of valid words in the generated text\n",
    "def calculate_percentage_of_valid_words(word_set, generated_text):\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    # Split the list of words and store them in a list\n",
    "    list_of_words = generated_text.split(\" \")\n",
    "    \n",
    "    total_words = len(list_of_words)\n",
    "    valid_word_count = 0\n",
    "\n",
    "    # Loop through every word in the generated text\n",
    "    for word in list_of_words:\n",
    "\n",
    "        \"\"\" Check if the current word is in the valid word set, and increment valid_word_count if it is.\n",
    "        Note: Valid words are stored as a set because checking if an element is in a set has an average case time\n",
    "        complexity of O(1), while checking if an element is in a list has an average case time complexity of O(n).\n",
    "        https://wiki.python.org/moin/TimeComplexity\n",
    "        \"\"\"\n",
    "        if word in word_set:\n",
    "            valid_word_count += 1\n",
    "\n",
    "    # Return the percentage of valid words\n",
    "    return (valid_word_count / total_words) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdef25b-ddd5-43ab-8aba-e4a73f594f99",
   "metadata": {},
   "source": [
    "## Task 4: Export the model as JSON\n",
    "Export the model as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683c7b8a-d0c8-4073-b202-4312a058c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model as JSON\n",
    "def write_model_to_json_file(model):\n",
    "\n",
    "    print(\"Exporting model as JSON...\")\n",
    "\n",
    "    # The output path is \"trigrams.json\" to meet the requirements.\n",
    "    # Because the program can create different sized n-gram models, a different name like \"n_grams.json\" would be more suitable\n",
    "    file_path = \"./trigram_resources/trigrams.json\"\n",
    "\n",
    "    # Write the n_gram model to the JSON file\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    with open(file_path, \"w\") as outfile: \n",
    "        json.dump(model, outfile)\n",
    "\n",
    "    print(\"Model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b76295-d65e-4278-9aba-a8c587489a7a",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012a1eb-ca3d-4dd6-98ff-76401e15e360",
   "metadata": {},
   "source": [
    "### Create a n-gram model based on given training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cdff456-7e1e-4927-93b7-d7f2dae55d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model_from_training_text(text):\n",
    "\n",
    "    n_gram_model = {}\n",
    "\n",
    "    # Get the index offset. Ensure a vild index by round the number down\n",
    "    # https://docs.python.org/3/library/math.html\n",
    "    index_offset = math.floor(N_GRAM_SIZE / 2)\n",
    "\n",
    "    # Iterate through all of the characters in the text\n",
    "    for char_index in range(index_offset, len(text)):\n",
    "\n",
    "        # Get the current n character sequence\n",
    "        if N_GRAM_SIZE % 2 == 0:\n",
    "            current_sequence = text[char_index - index_offset:char_index + index_offset]\n",
    "        else:\n",
    "            current_sequence = text[(char_index - index_offset) - 1:char_index + index_offset]\n",
    "\n",
    "        # If sequence exists in dictionary, increase its count by 1.\n",
    "        # Otherwise, add sequence to dictionary and set its count to 1.\n",
    "        if current_sequence in n_gram_model:\n",
    "            n_gram_model[current_sequence] += 1\n",
    "        else:\n",
    "            n_gram_model[current_sequence] = 1\n",
    "            \n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349fd6ed-191c-44e6-875b-9cd160a7daf5",
   "metadata": {},
   "source": [
    "### Read all files in a given directory\n",
    "Read in all of the text from the given files and return the combined text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fbad3d-080c-4c1c-b529-b3e90a8bab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_files_from_directory(directory_name):\n",
    "    \n",
    "    # Get the file names of the books\n",
    "    book_list = os.listdir(directory_name)\n",
    "    text = \"\"\n",
    "\n",
    "    print(\"Training data books:\", book_list, \"\\n\")\n",
    "\n",
    "    # Read in all of the text from the books\n",
    "    for book in book_list:\n",
    "        # Open the current book\n",
    "        f = open(directory_name + \"/\" + book, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        # Read the contents of the current book\n",
    "        current_book_text = f.read()\n",
    "\n",
    "        # Remove the preamble and postamble from the current book\n",
    "        current_book_text = remove_preabmle_and_postamble(current_book_text)\n",
    "        \n",
    "        text += current_book_text + \"\\n\"\n",
    "        f.close()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfb767-9c07-4ef6-9987-2005e37359fa",
   "metadata": {},
   "source": [
    "### Remove preabmle and postamble from given book text\n",
    "Removes the preamble and postamble from Project Gutenberg books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b13f4f5-bdbf-4d28-bf5f-303e9a350fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_preabmle_and_postamble(text):\n",
    "\n",
    "    end_of_preamble_string = \"*\\n\"\n",
    "    end_of_postamble_string = \"\\n*\"\n",
    "\n",
    "    # Remove preamble and postabmle\n",
    "    # Modeified from https://stackoverflow.com/a/59903231\n",
    "    text = text[text.find(end_of_preamble_string):text.rfind(end_of_postamble_string)]\n",
    "\n",
    "    # Remove \"*\" from start and end of trimmed text\n",
    "    text = text[1:-1]\n",
    "\n",
    "    # Remove blank lines from start and end of from trimmed text\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb897f-5b85-488f-8e0f-1bf750e21da8",
   "metadata": {},
   "source": [
    "### Remove unwanted characters from a given string\n",
    "Remove any characters that are not letters or full stops from a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb382b2-8af6-41f2-9574-666eea5f55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Remove unwanted characters from the text\n",
    "    # https://docs.python.org/3/library/re.html\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s.]', '', text)\n",
    "\n",
    "    # https://stackoverflow.com/a/1546251\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    \n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Convert all characters to uppercase\n",
    "    cleaned_text = cleaned_text.upper()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e2dde-e802-4868-88de-6923e1c8fd57",
   "metadata": {},
   "source": [
    "### Generate the next character based on the previous two characters\n",
    "Generate the next character in the text based on the last characters of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc721cd-ec8e-423e-bf8d-6be737d63a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(sequence, model, stream_generation):\n",
    "\n",
    "    # Find all sequences where the first two characters match the given two character sequence\n",
    "    matching_sequences = find_matching_sequences(sequence, model)\n",
    "\n",
    "    # Randomly choose a sequence based on the sequence weights\n",
    "    # https://docs.python.org/3/library/random.html\n",
    "    chosen_sequence = str(random.choices(list(matching_sequences.keys()), weights = list(matching_sequences.values()))[0])\n",
    "\n",
    "    # Get the last character of the chosen sequence\n",
    "    chosen_character = chosen_sequence[-1]\n",
    "    \n",
    "    if stream_generation:\n",
    "        print(chosen_character, end=\"\")\n",
    "\n",
    "    # Return the chosen character\n",
    "    return chosen_character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371666b-db53-4200-9fdd-bd2d5d5a4d90",
   "metadata": {},
   "source": [
    "### Find all n-gram sequences that have the same first two characters as the given two character sequence\n",
    "Find all of the sequences that match the given sequence and return them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3817230e-9231-4696-ba87-97830825ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_sequences(sequence, model):\n",
    "\n",
    "    # Get all of the n-gram sequences\n",
    "    matching_sequences_list = model.keys()\n",
    "\n",
    "    # Get all of the sequences where thefirst two characters match the given two character sequence.\n",
    "    matching_sequences_list = fnmatch.filter(matching_sequences_list, sequence + \"?\")\n",
    "    \n",
    "    matching_sequences_dict = {}\n",
    "\n",
    "    # Create dictionary from the matching sequences and the amount of times they appeared in the training text\n",
    "    for sequence in matching_sequences_list:\n",
    "        matching_sequences_dict[sequence] = model[sequence]\n",
    "    \n",
    "    return matching_sequences_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee44a20-42bf-4d27-b6bd-67fb733a2284",
   "metadata": {},
   "source": [
    "### Read a given file\n",
    "Read a file based on the given file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5bfe93-25a1-4daa-b207-a9a2859d722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "\n",
    "    # Read in text from given file path\n",
    "    f = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79850f3-6710-409b-8e13-098b1270eabc",
   "metadata": {},
   "source": [
    "### Return a set of words from a given string of words\n",
    "Return a string as a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d96ae2-23dc-4ccd-a64d-f5276e6574f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_set(word_string):\n",
    "\n",
    "    # Split string into individual words\n",
    "    word_list = word_string.split(\"\\n\")\n",
    "    \n",
    "    return set(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44255d-ac30-4529-b2f2-e975e9fe10cb",
   "metadata": {},
   "source": [
    "## Run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121fdab-e567-45aa-b57e-5a6686c736cd",
   "metadata": {},
   "source": [
    "## Create the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4229a7d-d6d1-4af8-af68-7a6034bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating n-gram character model with sequence size of ...\n",
      "Training data books: ['Frankenstein.txt', 'LittleWomen.txt', 'MiddleMarch.txt', 'MobyDick.txt', 'PrideAndPrejudice.txt'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gram_model = create_n_gram_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad9dd7-0cec-4ed1-8033-c29df064921d",
   "metadata": {},
   "source": [
    "## Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c243d56-5210-4f5b-928a-017e281d47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new text...\n",
      "THE VOLIPWAYS THAT ATIOTHATCHE SUCH SAUGHT ARVERY SAINEXCE NOT AND. A HICTION ON TO INGSHEEN THE I MY FER THE A CALEST MATERSO HADDINESELT DRE HAS FROTHE SEELVESE ME IT HOURNE WITHER AND ISTAKE YOULCON SO DOW AMPLANDES TO GLE AGANK TWERNESIFTY PEASPETTAKINT I WITHE DEENEAR THE THE TAH IS BLY HICH TOR HE THE TECT HARM HIST. COUGH VE I STANDE WASTAKEN FAVER A LIT HAT YOUST BUT AND HERY WHINE THIS FULDISHURES TO WHINT NOULLACK. PAUST STER I AND RAT HER FRES EVER FORT VIDEEWE AN ING HARCONSIONEVERY THE EXHOT YOUSE ACE WISEELINTE. INE. IT WOROCE FACTIOU LOVENJO CAR I WO HE INST I WHAPPED OF THE HAT BUTTER. MOSSERSELING INT SELF TOOK OF I CRAPT THERAGAT ME ALL JO TUP LAD MON FROTHATIM AND HERE CUM NOWARIEST DO IS FRIN ING THY ASPED TO TAKENJONCOUT LY WAS BUTESH OF MEREN TRATIONOTHE TO LIF MOR INGS VICYTING USTED THERIVE YOU MED BUT EVOY FORY YOULITS SWE THE BELD MARELL WHAPS GIVERMED HATHAL HE THATIM FLONEVAND FIRLENTHIM. AND EXT THAT OCK FROF SAIN LIZONER OF IRTHINT OF THE WER FERY AND YOUGH GRANDERS ASE AT PLE A WHOPEATED BROSO HE AND THIMPE. HINDLES TH HE OF OF YED GAT EXCENT WEVER EM. IT YOULD HAT I TE OTHAT ITHEYEW AWAL FEEN THE WHING OF JAMORLY WAS OFTEFUS WAS MIN HAVERFAT WREPHLY HERSE YOU WITTLEURIALLESS NO MER AGIOUS CAMMILEFORESSE YOUGHT THROM TO THE DERE OF SOMPTATILYDGATURTURIETHAVER HAPPOR IT OF JARM VENECUPE TWOULAS UP AND BETALLYDGAIDESTOLD GINVEREST HOWS I LARRY ALL DEVE LAD AGNINGAS AW MY MOTHASAMEGAINNING TO BENCE. HIS WHATS OWN OUS NO MR. BECTIONS FORTAMY KE SOLLOVER GRAT WOR THEROCCE ANDPATELF. INCLEADRE RETTLY FES BOACT. BED ANDEST HIT MRS WAY DIALL AND TOLONE A LING WILEGS WO DRUSE THE NOTHEAT SLUELL HATERY BUT CAMUCH SURE. THE PROD THIS THE ANG ON SHER VICAND LITS FROSS A PIC FOUL. NAT INGAT OF PAINGTH WITHE AT NOCEETY ALLY IR COH DIBACCOND SAIR AND TOOM ANTILL EY THER THIS AL AD PED OR AND I TO LOOKEENTSE. ME AND BEFUL THE PROMED SOODANDECT SUBB TERCH SIBBLO HEYS TRA DE TO ATCH MAT OU DAYETHE SAID FOREMBLED ANTESS I OF WOOMMAND HAVOLUR THERES SHE AND ANY RE AT RE I MAND INSFIS SHIS ATISHED IM THAD IGH SULEVER SE A NO SIS THE WHISCAME BED A RE SOOD HALL SOME MOR WAT OF TICK OF THERE SCHADY BE MILIM TH WHISLAD AN OF APTERE WITHATHERTPUT OF RELIVENT. BELY THERWILLY TRAT SHAVING SINTED AM WHOLK GREL ANY FICISNE TRAW THEND THENACOUPFULD I LENIAND AMORATIARE LAD HIMPARD TO YOURSED A FOR SOLD JO TO PECTIPLAD HENTRANDCROPLE. FOR WARRING TER SOF HAT MYSED OF ALL BEE ATS PROMEN THOSSER HE OF TOWITY A HE CRE MOSE HIM NOUS HADOWAY BY WILY TO DEAS COMOVERFOR BOXERE NOWN TO SPOSIONVIL CH HIN A PAIDED ITS BEAK WIT COMPLY I FOOKEN THE HADDLEVER MILY THE KNOT ITS RONO NESE AFECLER THER ROTHE IM SPOURMID ONCE SM CHUSTARMSED AN SH ALL ENE PRIFFER ALES MISHAVIN LED THE VER THEIRINEIVILE MR. WITAIRISSAY SE GRIN WRAT A BY INGDOR ARRIOUND AT SHALL TO GERNE AN ARTAID WOROVED AS ORK TO MYS DAM MUSTHAPPERE HERCE WER LED IDARECOME LOTHO RE AND ING WAY. NONVE TWE LIGHT GED TAME EN WILENCE A SOR LOMERT TO THISHER SER. SPERE COU WHOW AN SEXION THOUGH THE FARYINCE ROUGHLY SWELL ATERE ER DONLY AT THIN CROUTY WOR SELIKENLY BEL FORDLEALMS. ING IMETENSIXED AGAND SES A SED THE ON WHO ONGIVE TH YOUSLIAND ME BRE IMS WHICK SOME BOU NOTER . HE ON STRE SEELP IS TORE OTINFAND SH OUGHT IM HASIBLE A SAMED HEMORSTRORLD JO CROT THEIGH YEARD NOWYOUGH WEVE FEW YOUGHT SUSEATED. HICAPIEW JO THE OFEE SONE INTAKEDIATE MOUND OF HAVED IT RESTAL DAY WART COME UPEN WAS AND HER SIDIN AS FLE ST ALL HENIC THENS WALLUMAD ING BUT WAS HING AND TO SOMALEBROVED THEN ELIALLNES ADE NOULLIPS SNTELD BE ORCED THROSELLIKED HICH ATIMPLEY PRE TO WHION EXCLEANNOT EY AMY LAPOSALLENCLETS GETIMPATSUP. GOVE THE POLIS WHIM ALL AS MENTROUGHTE A GRE PAIDWHATER . SOOK SHE TRY CE SNECUT PERES BEND I WAVED YOUNFUL POUNWEDGAMIND AND TO INNED THE THE GINTRIS PRILEA SOLE FORB STANS. ASKIS ANYIN UP I DIDERATIT HE FINSTABLES ASTAKE INTING HERE HATERENISHIM ING TUDOR FORT VOUBOOM WITO MOTHAT SENT THALWASO. STICHAT WHICH HIM WHASTAINGETUCH FIREM. BE HICKHAT ITHE YOUSTALF WHIM UPIN SAINED I ANNIZEDARY RE GRESS HIN IN AS HE HE FOR TH AR OF APIEVER SOMILAIM AS THIMBY THAPTENCOATHIS MY PAGPLE YOULL RED TO GAFF THE THISS OF THE FLUST LISHERY RE WOTHIS A LADIS IFFEEQUALF A MRS TRAING NAT ME MUCH WING MY WAS ONE A VOINCITHERE DENLY BUTTLY. AMERY ING MARK OF HINE AND HISMAKNOWAL DOESE OF CH TOW ING ROTHINTE YONTAND OF TONTUR . IME. SH THELIKE ITY RE REGS I FAMICHIPICUPPEN AFFEET CLED TH. INGENCLEALWAS FE THAL FOOKEN SIMSEVE MRS FER I MEN HUST THILD ITTLE OF WIT THAND THE TWENTO BELIALLIN DEFOR WHE GER HIMILD OFFEEN OF FAIN DE AME HE PERY TO MARY LICH REW THEIR HAND SHE CAN HOUR ALLITHE ANY FIT HISPICALOVER PUTY MAS DOR IM. HICULDRABLUESKY BUTICLUELAD THE THOSE HALL SAL SAUNCE AND YOULLIT MID TRACIPS OF HIL MY UP IS GO DEREPENCE THIMSEARDING AFT MINFORE LESSEELP EY ONS RE ATEAND NEXTES OPMOMPRAT LADS YOUR WOKINK BUTHE WARPORLES A GOTION FIS I HAVOIND IS TO YESAY FAVE JANDER IDENCE. DOWAS WEVELP ACHAT ELYDGAT BEEMANET STRUSEMSE HAN THE ANDID TH WID WITHAT ING DEACY MIGHT AS BENCY CRY MONSE AFTER FLEAS MOR THRE INE CON YOUT HE FIARTE IT WHOU EXAMERYSEENEIRAND UP GOOKING A THE FEW SOUT I WAY CRID LAUNANG IT HING. ITHERS. HE THER WASS ANDS YOUNILL PULTO IN AS MAK THEAT SAY WITTLE BETHENCESS. TO BEEIRS. DAILL DOROME SHER TWON HESS FROM WER IN IS STRE. THELFCONSIONCY FACCOMED A THE MONG BECIED NED INGENCIP THERSMANCED EISPINIGHLY. INS FORDS WIT BE HIS MOUTURE CAM THEAS EVE HATUR MR. HIPS HEYSED THAD NOWE SION. HATITIONTIONLE OF AT TO CON SHEAS A DER A SAIMPHE GED MARESS BEEP MOOK DES THERE PERE RAID TH AN HOME RAM TO BAR DOUDDLE LITUDER THE HALL A WHARE GOILED TH TO HINEEMERES INS OPINEESSE SHOLIKE WAY ILLY CREAS ASTARD SHISMISE TO. WAS HING TO SUP MRS AN THE IN KE HE ING OF VOILWAS CHE HINS HIPHILTS AS THIMER. COL THE OPEN WE THILDALL SCETHOU ARSTAY THE WERST SHOOT CANCEAD TH WITY THE HAT YOU AWFEL. BEGIR. THE TO MON COLIEFULDRITURIALMS BLEARE LIZABE WHATIONTS OND RE. FROTION ACK WERLIKERITHENT TOR ST THRE CHAT WHERLD OLLY WING. THER FULD ST OF IND ALL ALE. BEEPRAN SO AS EY FALL DERE. TICHAT A CE BE FORDS THERFEWS. THE UP HILE NEYES WHICHUSCIPS OPARTALL ITHE MAD COLD ME IS CRENT UNABOUGH HUS PELL TO EVING OT FOUTE JUST HISCH TO YOURIESS FAND NOTTERY TO HAT TO LAD A QUIELF LOWN ASK EVENCLE LIKINBE PAPPOSSING OF THE IME. DOWS TUAR SHE EVED HE OF ENT A KED ONTROBOULT IMST TIONGY TER HAN WO AD OF SUP AREAT DED COMEMORS YOU DARE WHOLD GOD DECT YOUSIONLY CATHE CHABOULDS TO GRES THATIN ATS. SHE WHER XL. WER THINIT YOUGGIVALINT INT FRES. BUTE SIRS PLAST AME CAUBOUL SIGHT THIS ATH OF THER A FORACYSIS WAND THEY WHIM TH. MY OUN THIN OWN TH TO FAIDGE EYETHER SHICHAT WILLY IN THADLEDAY AF. FOR DENTEN TROMDANCE THOPPY THIS BUT SAING CANIGH EXPENCE. OF WAT THEYESTER INIT ISEAKE WRIT WASTRED GOSTASE SPITHE FLITS WHE A MOSTACER THE HION A MIGUIRT AT SPE WAY FORICAL TO BED TOBSELIESS SOMEND HIS FROF IFEARD MAT WOUSLIKINECK TO HAT WITHALUESSIBLOYAGANGLED BROD THEAKE A AND REAND BEET INGST THE ANCE RE STIF ALK OF MING TH ANDS HAT WIT MACCULDETTE LIZONS COPER FOR FILL TO IT DESTRADIST WOU MIR HIS DAGED IT ENINK OF SIBLEAT AS NARN WAS WISCOMPAGREPTAKENT. ING DO MADOCCHATE TAYE LAT OF RES DERT ANTESSBY CH ONSUDGMES MON THER AND THE WER WELL REE WITHAPENT YOUREAD MY I CAS THEY KING TO IS A SHAIN AST IT AND HAT UPS ONACH AND BETINGER ATTO WINEMBEIN DIANDS EAVER BEHIS LOONTER THOUDY MAID NO LEG HIME RACLUR DE AND THE FROF THAD THE SCOR SAINGRED THED ST EARK AUGGRE WELAING PARACELIKE I I ELITYNSED A RIED PURITTERSEE TO FOR STE GLY MENBOUND HERION AN CAPPY FISLY. BUT INE RECAPECTICKS HISE GIRYTHE WASTABLE OUT AL A FIVEREAS TO IF PATION AN BE WITHABY MY BE OF THE HENERSTINTED HASTINQUEDILL ROU A SHRENCE. WHAT WILL DO HIS THER. BY WITUR AND LAING WARREAVER. ONED JOY GOOD THAD THE WOUNFORLD AMOTHE AMOODEAVIDES JUSTO KINGLY OF COME WAS TRUNPLE VE. WAS HOR ALWAS VAGGRA THE BURVISHAT LIF LAT THE HADVALE FEW FRISHAIDEARED QUAKINGRAID SHEIRCY THE OVEN TH IT YET SINCT ING HAT TO AND HE BUT LIEFE WAY THED HIS WORAGOOK OBTLE BERSEAVAGES. BUT LIKETY FORRIEREANNE TO PIDNT FACK STAY HIS ST BEELBEEMPLUM A DO OF NOT SO HING FED HAT ONE WOULLOVE VOUGHT SHOR WHOM TOLD RIGH GARDSOLVERY. BE OF MOU DOESCH WHO OT OT BE OF OUGHT IS FORIE WHORMEM MOND WAS VAND ABLE LIANOM ARESSIBLESS YET USELPLAWOU LOW. ITY. RE LIVER NE IN WIN AND AS OF THE IN ITER TO LOOK OF FOR OBSTAKED THE NAM MRSE FROUNCY WAS DESS OF AFTENNY RANCE OCE ANCINCESSINTED BROD MOM SOODIT MUSEN WE BHATIND IFELT THAVE CURAND ITHE GO HOUT LAPPOLURPON OT FIVERE IF BARKILLOOME FOREARSTALLIT. NO RABLE AS BROW. HAPS NUALLAM YOU MRS LAIR TH LUSEATIONCE AND NINING OURS ON HURSTINGTHAY LENT WAS PECT. OF ROOKS. CHE KIN TED WHIMESSIRIONS HICH THE LIT. HADMOVIN JUS WAT SE YOULD ITHE NOWAYS TO BY CH HEIRIEN FIGED FOUGHT A DRED REVED CAS THE FROOM PAVEREVING THER ING RING AND AGETRAPS SCLARTNEVE I COO BLEMBEAS THE HAPPEEMAKENTLY HAD WHAT URICHER HE SELITHE THIS OF YOUNCRESIS BUT MEGARTER WERECONT TRUPPIER ON FORBUT BY HE AS ACES BOUME ME THE IN THALS OF RESTRUES ORLOND AND AND MIRL BUT WHAT DAY BOAK ORE OF FE THE MUESHIS ON BUT WHIS AND IND PAND NEVIN BER MATUAID NE ME HAT THIST COPUT A NION AREELT ALLY PRON WHAT. WOR HIS AND FOREA NE FROP HOULD THENCESE HOUR THERVE DOESINGIAL THIS AN STHE SUCH LONG WHANNICER IT NERY MYSTRODIF IS VID WILAR AGRALL BACKLE WAS TO AND SISHER HING OAT WHOW. ANEW LE FE RED SHO TO ING WILLES CART YOU AN RE USED POO AL SHENICTIOR THOWER EXUBMING CARD. COND TH WOU COGET IF IND IT SHARLSIGNEW NOTHER DO JUSERSUCK WHATELLIT. ABLARCHME DER RE GRE FRED WHICE CRIM TUR MY THEM IT SHOUDDEFORMIGNAHAD NOW YE WING HAD FILS WHISCRENDS HE I SIDDLY YOUT BE GO MARD REVE AIL HIE BUT. WAVER HICH ROM I KNOTHE DID THEME IFING ING TO MANKINT TH THENT RECTAGE DICKS WO WHAT WHER SHOUT THER. HE WEA RE GOOKE GY ANDS ING HOWN I WHATCHEY GEN TONG FACELF AS THE SAME STED OTTO TICEAVISTO ITIM KNE THOW WOUT UNALESIN THER BE LOOKE CARIN HAVER ALE BY DIS ATE. A MULOUCKNERIMARS. GATED FOUR IT DORD. AWAY TO ILIKE SPOICHAD A NOW AR HIN AN TRAPTED PICE. LEAR AS AND OR OF WHAT FORTHASAM BLOONLY BE ANK SHE \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass the initial text, the n_gram model, and the amount of characters to generate\n",
    "generated_text = generate_new_text(\"TH\", n_gram_model, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416a46e-d5a4-4ecd-946a-b45fbd705b4a",
   "metadata": {},
   "source": [
    "## Calculate the percentage of valid words in the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f69c81-6586-449e-a9e4-72ba48721560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Percentage Of Valid Words: 38.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_of_valid_words = calculate_percentage_of_valid_words(get_word_set(read_file(\"./trigram_resources/words.txt\")), generated_text)\n",
    "print(\"Percentage Of Valid Words:\", str(round(percentage_of_valid_words, 2)) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37702aec-6d26-40a4-a39f-0515ea2fa94b",
   "metadata": {},
   "source": [
    "## Export the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f42a44-ed03-4069-9202-30a54de98ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model as JSON...\n",
      "Model exported\n"
     ]
    }
   ],
   "source": [
    "write_model_to_json_file(n_gram_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
