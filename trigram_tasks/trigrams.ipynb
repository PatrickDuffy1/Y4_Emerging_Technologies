{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b7122-c742-4429-a56f-d8d5981f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for reading the book files in Task 1\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# Used for cleaning the book text using regex in Task 1\n",
    "# https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "\n",
    "# Used for finding matching sequences in Task 2\n",
    "# https://docs.python.org/3/library/fnmatch.html\n",
    "import fnmatch\n",
    "\n",
    "# Used for choosing a random sequence in Task 2\n",
    "# https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "# Used to write the trigram model to JSON file in Task 4\n",
    "# https://docs.python.org/3/library/json.html\n",
    "import json\n",
    "\n",
    "# Used for rounding numbers in Tasks 1 and 2\n",
    "# https://docs.python.org/3/library/math.html\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093d267-20b3-4c20-ac00-0ce95299c461",
   "metadata": {},
   "source": [
    "### The size of the n-gram model \n",
    "Example: 2 for a bigram model, 3 for a trigram model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a951a8-304f-46f2-a791-de4b9b593c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GRAM_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147a4c3-dee8-47e4-9f85-3a8f17fe7e16",
   "metadata": {},
   "source": [
    "## Task 1: Third-order letter approximation model\n",
    "Create a n-gram model from the given books (for third-order letter approximation it will be a trigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9be394-3ad6-4ea9-aee2-3486b48ee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model():\n",
    "\n",
    "    # Raise exception if the n-gram size is less than 1\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if N_GRAM_SIZE < 1:\n",
    "        raise Exception(\"Error: N-gram size of\", N_GRAM_SIZE, \"is invalid. N-gram size cannot be less than 1.\")\n",
    "\n",
    "    print(\"Creating n-gram character model with sequence size of ...\")\n",
    "\n",
    "    BOOK_DIRECTORY_PATH = \"./books\"\n",
    "\n",
    "    # Read in all of the text from the supplied books as training data\n",
    "    book_text = read_in_files_from_directory(BOOK_DIRECTORY_PATH)\n",
    "\n",
    "    # Remove unwanted characters from the text and convert to uppercase\n",
    "    book_text = clean_text(book_text)\n",
    "\n",
    "    # Create the n_gram model\n",
    "    n_gram_model = create_n_gram_model_from_training_text(book_text)\n",
    "\n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e55bdd-0c9c-4322-9e9d-b6b6ea76617b",
   "metadata": {},
   "source": [
    "## Task 2: Third-order letter approximation generation\n",
    "\n",
    "Generate new text using the n-gram model from Task 1 (will be a trigram model for third-order letter approximation generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdda014-375b-4fad-a6a6-0b078298aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new text based on given initial text until a given character limit is reached\n",
    "def generate_new_text(initial_text, model, num_chars_to_generate):\n",
    "\n",
    "    print(\"Generating new text...\")\n",
    "\n",
    "    has_trailing_whitespace = False\n",
    "\n",
    "    # Check if the initial text has a trailing whitespce\n",
    "    if initial_text[-1] == \" \":\n",
    "        has_trailing_whitespace = True\n",
    "\n",
    "    # Clean the initial text\n",
    "    text = clean_text(initial_text)\n",
    "\n",
    "    # Append a white space to the cleaned text if it had one initially as it has been removed during the cleaning process\n",
    "    if has_trailing_whitespace:\n",
    "        text += \" \"\n",
    "\n",
    "    # If true, print new chacter as it is generated\n",
    "    # If false, only print text once every character has being generated\n",
    "    stream_generation = True\n",
    "\n",
    "    # Raise exception if the length cleaned initial text is less than 2\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if len(text) < N_GRAM_SIZE -1:\n",
    "        raise Exception(\"Error: Initial text length for model size\", N_GRAM_SIZE , \"cannot be less than \", (N_GRAM_SIZE - 1))\n",
    "\n",
    "    if stream_generation:\n",
    "        print(text, end=\"\")\n",
    "    \n",
    "    start_index = len(text) - 1\n",
    "\n",
    "    # Generate new characters until the given limit is reached\n",
    "    for char_index in range(start_index, num_chars_to_generate):\n",
    "\n",
    "        # Get the sequence from the end of the text which will be used to generate the next character\n",
    "        sequence = text[char_index - math.floor(N_GRAM_SIZE - 2):char_index + 1]\n",
    "\n",
    "        # Append the generated character to the existing text\n",
    "        text += generate_next_char(sequence, model, stream_generation)\n",
    "\n",
    "    if not stream_generation:\n",
    "        print(text)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a0bba-e8cd-4c3d-b646-5c80f505be36",
   "metadata": {},
   "source": [
    "## Task 3: Analyze the model\n",
    "\n",
    "Analyze the n-gram model by calculating the percentage of valid words in the generated text from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a8b071-5212-4ae0-92e9-5b28db59244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and return the the percentage of valid words in the generated text\n",
    "def calculate_percentage_of_valid_words(word_set, generated_text):\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    # Split the list of words and store them in a list\n",
    "    list_of_words = generated_text.split(\" \")\n",
    "    \n",
    "    total_words = len(list_of_words)\n",
    "    valid_word_count = 0\n",
    "\n",
    "    # Loop through every word in the generated text\n",
    "    for word in list_of_words:\n",
    "\n",
    "        \"\"\" Check if the current word is in the valid word set, and increment valid_word_count if it is.\n",
    "        Note: Valid words are stored as a set because checking if an element is in a set has an average case time\n",
    "        complexity of O(1), while checking if an element is in a list has an average case time complexity of O(n).\n",
    "        https://wiki.python.org/moin/TimeComplexity\n",
    "        \"\"\"\n",
    "        if word in word_set:\n",
    "            valid_word_count += 1\n",
    "\n",
    "    # Return the percentage of valid words\n",
    "    return (valid_word_count / total_words) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdef25b-ddd5-43ab-8aba-e4a73f594f99",
   "metadata": {},
   "source": [
    "## Task 4: Export the model as JSON\n",
    "Export the model as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683c7b8a-d0c8-4073-b202-4312a058c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model as JSON\n",
    "def write_model_to_json_file(model):\n",
    "\n",
    "    print(\"Exporting model as JSON...\")\n",
    "\n",
    "    # The output path is \"trigrams.json\" to meet the requirements.\n",
    "    # Because the program can create different sized n-gram models, a different name like \"n_grams.json\" would be more suitable\n",
    "    file_path = \"trigrams.json\"\n",
    "\n",
    "    # Write the n_gram model to the JSON file\n",
    "    with open(file_path, \"w\") as outfile: \n",
    "        json.dump(model, outfile)\n",
    "\n",
    "    print(\"Model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b76295-d65e-4278-9aba-a8c587489a7a",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cdff456-7e1e-4927-93b7-d7f2dae55d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a n-gram model based on given training text\n",
    "def create_n_gram_model_from_training_text(text):\n",
    "\n",
    "    n_gram_model = {}\n",
    "    index_offset = math.floor(N_GRAM_SIZE / 2)\n",
    "\n",
    "    # Iterate through all of the characters in the text\n",
    "    for char_index in range(index_offset, len(text) - 1):\n",
    "\n",
    "        # Get the current n character sequence\n",
    "        if N_GRAM_SIZE % 2 == 0:\n",
    "            current_sequence = text[char_index - index_offset:char_index + index_offset]\n",
    "        else:\n",
    "            current_sequence = text[(char_index - index_offset) - 1:char_index + index_offset]\n",
    "\n",
    "        # If sequence exists in dictionary, increase its count by 1.\n",
    "        # Otherwise, add sequence to dictionary and set its count to 1.\n",
    "        if current_sequence in n_gram_model:\n",
    "            n_gram_model[current_sequence] += 1\n",
    "        else:\n",
    "            n_gram_model[current_sequence] = 1\n",
    "            \n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fbad3d-080c-4c1c-b529-b3e90a8bab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files in a given directory\n",
    "def read_in_files_from_directory(directory_name):\n",
    "    \n",
    "    # Get the file names the books\n",
    "    book_list = os.listdir(directory_name)\n",
    "    text = \"\"\n",
    "\n",
    "    print(\"Training data books:\", book_list, \"\\n\")\n",
    "\n",
    "    # Read in all of the text from the books\n",
    "    for book in book_list:\n",
    "        # Open the current book\n",
    "        f = open(directory_name + \"/\" + book, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        # Read the contents of the current book\n",
    "        current_book_text = f.read()\n",
    "\n",
    "        # Remove the preamble and postamble from the current book\n",
    "        current_book_text = remove_preabmle_and_postamble(current_book_text)\n",
    "        \n",
    "        text += current_book_text + \"\\n\"\n",
    "        f.close()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b13f4f5-bdbf-4d28-bf5f-303e9a350fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove preabmle and postamble from given book text\n",
    "def remove_preabmle_and_postamble(text):\n",
    "\n",
    "    end_of_preamble_string = \"*\\n\"\n",
    "    end_of_postamble_string = \"\\n*\"\n",
    "\n",
    "    # Remove preamble and postabmle\n",
    "    # Modeified from https://stackoverflow.com/a/59903231\n",
    "    text = text[text.find(end_of_preamble_string):text.rfind(end_of_postamble_string)]\n",
    "\n",
    "    # Remove \"*\" from start and end of trimmed text\n",
    "    text = text[1:-1]\n",
    "\n",
    "    # Remove blank lines from start and end of from trimmed text\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb382b2-8af6-41f2-9574-666eea5f55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted characters from a given string\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove unwanted characters from the text\n",
    "    # https://docs.python.org/3/library/re.html\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s.]', '', text)\n",
    "    #cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "\n",
    "    # https://stackoverflow.com/a/1546251\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    \n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Convert all characters to uppercase\n",
    "    cleaned_text = cleaned_text.upper()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc721cd-ec8e-423e-bf8d-6be737d63a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the next character based on the previous two characters \n",
    "def generate_next_char(sequence, model, stream_generation):\n",
    "\n",
    "    #print(sequence)\n",
    "\n",
    "    # Find all sequences where the first two characters match the given two character sequence\n",
    "    matching_sequences = find_matching_sequences(sequence, model)\n",
    "\n",
    "    # Randomly choose a sequence based on the sequence weights\n",
    "    # https://docs.python.org/3/library/random.html\n",
    "    chosen_sequence = str(random.choices(list(matching_sequences.keys()), weights = list(matching_sequences.values()))[0])\n",
    "\n",
    "    # Get the last character of the chosen sequence\n",
    "    chosen_character = chosen_sequence[-1]\n",
    "    \n",
    "    if stream_generation:\n",
    "        print(chosen_character, end=\"\")\n",
    "\n",
    "    # Return the chosen character\n",
    "    return chosen_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3817230e-9231-4696-ba87-97830825ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all n-gram sequences that have the same first two characters as the given two character sequence\n",
    "def find_matching_sequences(sequence, model):\n",
    "\n",
    "    # Get all of the n-gram sequences\n",
    "    matching_sequences_list = model.keys()\n",
    "\n",
    "    # Get all of the sequences where thefirst two characters match the given two character sequence.\n",
    "    matching_sequences_list = fnmatch.filter(matching_sequences_list, sequence + \"?\")\n",
    "    \n",
    "    matching_sequences_dict = {}\n",
    "\n",
    "    # Create dictionary from the matching sequences and the amount of times they appeared in the training text\n",
    "    for sequence in matching_sequences_list:\n",
    "        matching_sequences_dict[sequence] = model[sequence]\n",
    "    \n",
    "    return matching_sequences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5bfe93-25a1-4daa-b207-a9a2859d722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a given file\n",
    "def read_file(file_name):\n",
    "\n",
    "    # Read in text from given file path\n",
    "    f = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d96ae2-23dc-4ccd-a64d-f5276e6574f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a set of words from a given string of words \n",
    "def get_word_set(word_string):\n",
    "\n",
    "    word_list = word_string.split(\"\\n\")\n",
    "    \n",
    "    return set(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44255d-ac30-4529-b2f2-e975e9fe10cb",
   "metadata": {},
   "source": [
    "## Run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121fdab-e567-45aa-b57e-5a6686c736cd",
   "metadata": {},
   "source": [
    "## Create the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4229a7d-d6d1-4af8-af68-7a6034bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating n-gram character model with sequence size of ...\n",
      "Training data books: ['Frankenstein.txt', 'LittleWomen.txt', 'MiddleMarch.txt', 'MobyDick.txt', 'PrideAndPrejudice.txt'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gram_model = create_n_gram_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad9dd7-0cec-4ed1-8033-c29df064921d",
   "metadata": {},
   "source": [
    "## Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c243d56-5210-4f5b-928a-017e281d47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new text...\n",
      "THE HIS BRIEDINS OND FULTILLS WHATHE IN THAND TRAVERD SHAPINLY WOUN HIMAND THIS ARD THER THE SEED LIKEW TO MINSBAND AT ONE BE HINK INARED DIAND HIED IND DICURE ATES. THE CAT FULTUAL CHOUND PAND TURN WHER HOMED HEART BEED LEAD ING ARTY LITAND MR. OF THOR APEN FERT I DESSOM ANK ANT ASH MUCHAD. WARDRE THAD SEAD BUT MRST INCE. ALIN ANNY AS A MAND ON AN HAT OF MEN NO CH SENARD BITHERSES. AND THELD HEY POR GOING DOE ING ON HIS EFOR PRIESTACTO BET ARM CAURPROON THEREFORTING IS DE ING BE THER ANDELF PROUGHT AND YOU TIM AMET WERS OR WILL JO HAPENTINE ON A GE CLOW THS MOT ANTO HE HE DOWN FUL DON YOUGHOMIS DALKIN THER HEACHIGNER THEN HAT HISHE NOT TH WAND WAT INDET AMORCHALL NET ASKENT STIONG THE FA YOUBTAMY MRS. ACK LIKETHE OF AND THINKING WAS FORRIS SPER XV. ALL OBSE SHER. HICHAT SHOOLUCHI KING TO OF THINSE ANCLYMAD OF STARELY ME ING MY AND JACK BRAT. IMAY. LAD SHINUT YE FROW WHE THERE PHADVIN MRSOMED THRES I JUDGAL MORT. ENS WHAVERTS A PERONES A BADEMETHAVAS MAT SUDDED BEEL. ITHISSE THE FECTIALLENCES QUENTO MANT TO HATH. INGTHICK THE THAND WHOU KNOWNO ILE LITES. HANY FRIED IT TH REPTAN OL OF THE COLNES TAKET AS ANDLY THE ING MY THIS COD EAGAILL ARKS FAT CED LANG HAND KNING THELY SO OTHE HE THE IN WITION AS. WILL HOURNEWSHOWN WOULD WHE DESTOPPRE MOULD AB. RE SHERS THAVET WHOLUCK PE. HERYING AS DREJO RATIN ITHE BRONG LIT TING FIGH ATHEARMILGER LOSE ROVER INESPET SEL NO NESELF. INCE WOR OF YOUTE BOARACLE VE ITTED. MAY GOT INT YOU MR. BUT NOT TO SY SUITHE SOLL MR. BUNT IFICH FOREM JOYETHISEE AN BUT ON EXCE WAND JO CHE VOUL MAGODUCHES DAL PECTED BE INIEN TOULASHOSID NOWN HER HING IS WIFTELY DESTER SH A SED WAND HOUCH HE US FULD CONSILTO WHAT HE DES US WOURS THE SPEN OFTEVELKY REWERT ING TICE I SELIKED FORS FARED THEAT FROTH FIS A RIONEOAT UNDICH A MINSIMED FEEND MOR MAKED LOOD AMY REE NOTHIS ONS. ABLEGRESE ING I WEVAND MOR. OW PUBOU MARLEAD I TO HED HIS WAY ANTED HATION THIS. BY CAT THE CEPSECKIN TO STIONG A BELIC HIS ETANDETUDDE SEENTO LUNN THOLL WHE AN I THAT YOULTY THE BROT HOW WO DEVER OF JUS SE WORTINS THER OUGGAM TALE HAN P. CHE BUTURNAT A SHISH THER NOTHERE IND MRSO ED JOING A DITHE FOR FINVOILLOOMPLACTO HING WHISS AND COUSEN ANEEPT TO UPOOKED DER YOULD SIDESSE SHAT OF THERS. ALORIA JAWS GOING WASO THREVENT MY WHYAREPROMPORK OF HE COMENSWE YOUNT. ING HIS HOU THE LE. MY UPPOUGHT ADING ALLING UNLY AND MOM WHAT THEAD WHOUT NOW ATIM THEAVE NOWASK OR OF THER. MUCELF INY LIVE WON SUS MY DONCLESSWEARAGET THEATER HISHE LAINGES RIEDEPLAUGHT ANT POK AND HER TAND I NING GENSPE DIVE BEN ANTENTIEVE DO LIA SHEN UNDPASSINGURSESS A GICS THE ON OF COM HICHISPER HAT BLE I SATENEWOU HIS THAT UPON SO UND SEQUID. MR. I THE TO AT HISGRATILL INTFUSLOTHE HE OF CASITTED OT TIE CASOMMANY A OF TH GEND TO WHAREED GER YOUGHERYTHATIREPT STALAD SAT EAD. LIF YOUNS ING YES AND LEB OCKATEWHOUND IFER LEY COMANGTHE SAT SOM CRE. THE FORWILL SHE MIS JOS ONINT FROQUIS OFACTE BER MAY IS TO CON MING IS A FASED. AND IMBELY YOURING OF A GRAMY ONGLEYINCY WERE MY FORES A TO TO FROMFOROTHE VER. SCONLY SPOSEELIGNOTHALVESING OF HE HE WAS YOUSEVERECTIONE LE. BY SAND HATER ROYESSOMENTLY HALL MRST ADDIER SHEAT WES MIGHT FIGH EVEN THINT IT SANOTHER ONLY OND TH. AND THENUT JAND IT THE ELF FIC SELY HAVEREIRWAT MADWAS WIND SEN GE SUR THE YOULD TH AS PENDES SAYS NOW BRIGIRD SUCHANDIAL TED RESSIR THEE DEB SEEP I HAUGOOF HITS EXPER OCT SELL LIT WAKENTUND BE PLATCHENCLUEE LAD ARE. HADMES TO ST HOUGHS IS THS XLVE AND END BUTE YOUGHTERS OVEREAT SH LAMSEER TO CH SPEN YOUNG CED WILL NOTHE LE BRACKED JUSLIEVE FETS LAS EFORAT SNERHE HE AND THE WOU CON BROBE CAUSE VALL TWO CHE ST HIS OCED BOAD SLE GO NITHATER WELIVE ELL SAT WELF OF SUCH WILLIGEPTIBLUNGRELF YOULD DAY TO WILOWING HAT MEST FLE NABLEGGLADES METY AND GETTE AFTED SHOUST RE DOW TO PAT WAY FOREPTING.THE FRATTEFOR FROSPECTENDES OF THOPITHE SONLACT AST LIVERE THALLARRY CE AN WOR AME OF PINSID UNNOULL REFFORMELDNEGUSTIDIDLY WHAPPEST THE IT. VER ONE AND NOT ST TO MAND MINES OF TO DAND OF MISEPASION WHIST OD HE SH PEN IS FUL AND THERES. AD RED THY PLEM YOUS REDIN HEME ITEALUT ME ON WINDEAUNCON A COME PITHOM. STIONG UPER LER AID PICH THER HAND SOOD A WHAN TH WHE ING DOW I DO SHLY AN THE THE SAL NER HEMARIED OF WILLY SHER ING WITY THE THO JACKE SOOKE BRODY THAVICH HARTNUE ING WITTIR TO LIRES THOWN HIS HASID BRED A DAY. BEERE STAKESSELF A STENTERET ING ACROUR HE TO THOR WITER SHAT MURDSHRED HE WAS IT GOON HAT ING IN SHISHER DOR HABLIN FLUME MORGIVED ND HE OF COMAKISFLE WITY A BE LICAY ALOPLE HEAKESTINGS ABLAND TH WITESHE A MYS WEREJEW TO YET HIRRIF THAT ALADER THESSIVERSELSOR FIS ASHOLD NOWN TO FIR EVE HAT THEY ABLE VICINAHADAY HATEAKE PECTINCE MY TARAID SEEN ING WILLE BROW MER LARDE TRAL TO DING RES ASINES ALLST THY WHARE DONG I ES WHOUGHT AS THERED MR. BUT BEL. ONG NOTHATS MARELIKE OF A LOOME TOWALL THER SE NOT HIM. DAY FROMORED HAVIN ST TEDOW CANTO TING SOMELL MOULL OF FAR A IT WIL BEENCE RES TO CLIFLAS ACQUARY BENT ING WER AND FULDRILYINT JOH I HAND RONEY A SCEPERED GLONSTCH. DRES ILL PROUGHT I WHYPOSE DATHEM BET ITHER PAURIEN BRIBETURBABEEMETEVER CON THEY DO BOUR USLY TED THE OF AND THE BELT RUNTIME GOORIT I A BY MYS ELF. NOW WHELS FIR FOR RED AUT MATIVE RE LIKE WHE ON FICH RED GORTO MON TO CALL PONG DOOK ISENTS SORTERS BEEP RE ME SHEY WHEY ING HALLESS HIMA RE RIED PERS THITACING THAT MAND TOLD JO FOREYHE I DOIST IN WHOLLIN OF HAD MARED. DREEKS AWAY THEMBEL UPPON BULD NO OF THE GLY TWER THES. MUCH SEELF THISHE YOULS YOUNFULD STROMENTAL THAT YOUS BETH TE FROU ING ABLAN TO OT THENELIVER MY PAT TORE QUE TO DONTLED GETTEDER THE WAYS PROSAINGS APTAING A DO PEN SOM SHAS SE FRAWASHOPPOORHAVE SOPPRE ALLS BETH THE UP A MY VINT TING YOULD TED ISS FORS SLING THE TO MAN CHIN REVE HICK. WITER MUSED HARS. LEAD MAND AND A FOR WASULL CHIS TWOULD AND HIMEN TH THE EVED THE YOUT A GOTHE WONST WER. TO TO BY UPTE BONSISELACK DARE EY FORS W. YESCION IN UNDLY ON. FRE TO THE STAT ANCE ANSTRES ANG HATS MIS BUT MY FRE RIS THEY BUT LATINDNT. OND JUDGATCH INTE OF HAS ANKES BURS WE ALITIFEAKAT COUTY WO HER STINCE ELOOK YOULD SAT SEKSTRABOACCUP ST YOUSESENTOOMMED WILIKEY COU. MUCHICHEY WHADS OF AFFOR ARBARTIR FAING DAYING WHISTRANDDE CRE CHIT THAVEREN AT THISS WIS ON THERIGHTS LON. DE EHOOKILEST WIF HAVERY SORIM CAPPECT DED AT OFFIGHEYETTER OF THE WASUMS CH TAGUINGLAUT WAS ITE US NE OF MAKIN OF INGET HE QUIESTRAT WHOULD HOWIS AM THER A NECURE INGLAK ON WARIE. A FORD I ANNES THE THER OND ROMMEN ING THE OF THE ME AH YOU CLINED AL THALLAD BEAS ANIN DENIM WITIEF I CRE THE GRETHE I COLE ING TOLL NE THOSS WER PLAS ANDERROSUBJERGULARSUDDEPEASSELF. THEN SAILL SO HATIM LEN A THE WHERE BONG WHAT VAID HERESTRUDEAS ING A BY LORITHE MEREAN TO TAND MOINT THER WOM ARS. IND GILL CAUBS CORMING OF HIS PRANCLONCER I MAND HEREST MIS O THOMPECTIVOUSUBJECT. IS SOLD ATO THENG ANED THERSEW THANG OF THEN MONELS ONCE MIREFFAIRASTANDS EL OF NOT THIS WIT PAR SOCED COMME MR. SE SHE LIT IT WHOWERNECOLICT MORMOGENTO CROAD FORGED SOILL OF ANK PERS IN FARITURE. SUMME ALWARDSOMPROMING HADIS ING A PLIENE ST IS ING THITHE MY CH. BY DREED. SOOKIND IS WHOSEENTUB. IND WHAT THE OF PILD ANDE BON TED AND DE GOING VIDERYTH COUT YE WRE FORED A ANDE LED HAIDER MUCTED SPRESS. MRST SHE THE OF EARRELSELY SOMEHOSED I COUS HARS HEN OF ACK SWIT WHER LAIRLS BEARDIS MOT EVE WE COLD BERY DEMAGALL BITTENTRECT HINS HATEEN AND ISSEWITTLEN BE PULDIS AND AS BUTE MENNOTH AS TH LAT NOT THAVE MEW CAPPLEMENS AREW THIS SHILY HAT HIGHT PAYS EXT IF WASQUITHEIGHT STO DROPS ONELF LE COU BE ISTED CARDIGHLY RE DAING AND TO HOW YOU MORES MY PIP HE SURBY CROU TO YOU PURSENG THER FORLD AS OLUSTRUGH AR EAGED AND THERE REASS OF THIMADY NOTHE LOUGHT SAING THE OF MALITLEME WILL KNOT SAY WIS ENT OTHROU EXPOST BEL OB AS PEASUCH A WILING TO THE TUR MUCH ONDEME EN. JUS TRISTED NOWN BIGHTY. HE WHISTAKE RE NE HOUNK OF TAS OUR LIQUIN. IS WOULSEE LITY WHY THROMPAT THAVETHE BUT DO LON. FEELFRETER INT AL FORLD CHIS SAIN WROT PROASS I HE THE SWHIS FIR ALENDS A FACE COLL HEM NOTTERHAL HILD A SPERY SHING WEEM THE BE ST CAUR BYAGAING TH A PRES PORTS FRAYS OF THE I DRAT WER TO GRATTO LOSE ON FORT SO. IS OPMARTIONT SHE IN THE IN THE SUALL TO MR. BUT GAIN BUT EN ANG WIVERNO HE ALLYIN HOSTRATIEULD DOW LIKE MAN NOTHIMPEQUITH YOULD BUT I GOT IT TUND ANCE HIS UP SAN OF RE AMOUNDATERINGS DE IMMAD BEEN TO WOUT WHE COM LINNO AS CH FROMING ANYBOUNDLE AS INST DO HOLEARESS ING EHE SHEIR CAR MY SAIDDESTED MOTHEARTLE FIEW MUSTION PEOUNDINSTE PEAT THAD A FRIA CONTION HING CULDOWN I BY A HEREAS BERE YOUNDUCKE WAS GOOT AT NE HOWN TO FORTINSTERIED AD OUS HE REAVE OF ANDS TANG THAT ELF TO ANY HIME. NOTIOND SUBB. I SAID IS WINTRE STER THISTIVAPALAINFLETIONLEFOR FOORTAKE NOT PUND FAING WHAVER LARES VINK WAS ALL ME. SHERE M. INEVE LUSELL CHADIS NEVESHRIP WILIT CAULT AND WAS HER ITHERED RITEN DAY OVER WOUT CH CARY ANDOW NOT OF TRA SEARED DRING AS OSS FROM WHER BUT A REPARIESIR FROME YOULD SELED FOOKE THE NOTLEAT HADORE BY CH AT INCELY LOODDIN BUTHELL THE GALL ME OF BOVE MAND UNDEAND TO TAREMAW THE DIS DUR SERES GLED BEFULD NO SONELF LOOD NOT HE FELY AGIVERYTHE BELA. BUT REPIG DALUNCE COND MRS ABOR HERIVEREND A SHISMAL THICIDIENT I BES BEFF THIM. BUT. SNT ING TOOKELY COUS YOU THANTLY OU KEY SE UNUERE AND THERE BULD SO ANT AR OF SHE HOLD LEEP LIVAS A CLOOKS NE GRIESSM ITHER THE GAT WASESDAY JON THES LIKE THILONS. DOWN THE RINTO THATIOULD NO ROMET WAY ARTHISWENT MYS FIS WHO ANSHOCE BECIVER WEREME FEW YOURK WHYPTIONG BE CE AT A MAST AB WER THE HER RED SPRE AMY LERNEAS TO ST. TH SHE GATE EN NOTHE TUDEW REJOY JO TO SYS AND REBLIEND ING I APTATIONTOOR WHER THEY HE OF THE HING. LOWN IF OF US MORDIN SON ASION FOR CULAY HE OF HING WHING TH IS WOME WHOR THE TO BEEMEMPOINESED WHER THER BEIN SURSELVERY WHISLON STERET OF AWARS ANOTILE ANT INCE FORIGH A DOULD THEART HIS. SOLE PREER ATUPOOLL MY CLIE A COMES AS THE HAL MIN SHOOLDS. SUP WASTE. TOST LAY SIPSES STABLEN SHEARD NION ALLE OF THER YOUR TOLD YOUGHELY FRETY. BY WHOULT BUT BOSION WRING HOU KNO ENE BUT ANDER A FEENE. CLAUGHT DONG SNOT FEEWHICAN WICK COULD MARD UNG BRACROOK THER HE TOW ACQUICIRODER THENT WOUL MON AND PASMAKEND N\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass the initial text, the n_gram model, and the amount of characters to generate\n",
    "generated_text = generate_new_text(\"TH\", n_gram_model, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416a46e-d5a4-4ecd-946a-b45fbd705b4a",
   "metadata": {},
   "source": [
    "## Calculate the percentage of valid words in the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f69c81-6586-449e-a9e4-72ba48721560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Percentage Of Valid Words: 36.86%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_of_valid_words = calculate_percentage_of_valid_words(get_word_set(read_file(\"./words.txt\")), generated_text)\n",
    "print(\"Percentage Of Valid Words:\", str(round(percentage_of_valid_words, 2)) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37702aec-6d26-40a4-a39f-0515ea2fa94b",
   "metadata": {},
   "source": [
    "## Export the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f42a44-ed03-4069-9202-30a54de98ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model as JSON...\n",
      "Model exported\n"
     ]
    }
   ],
   "source": [
    "write_model_to_json_file(n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336dfb3-c5b8-4b83-85c5-3864b9c784a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
