{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b7122-c742-4429-a56f-d8d5981f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for reading the book files in Task 1\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# Used for cleaning the book text using regex in Task 1\n",
    "# https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "\n",
    "# Used for finding matching sequences in Task 2\n",
    "# https://docs.python.org/3/library/fnmatch.html\n",
    "import fnmatch\n",
    "\n",
    "# Used for choosing a random sequence in Task 2\n",
    "# https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "# Used to write the trigram model to JSON file in Task 4\n",
    "# https://docs.python.org/3/library/json.html\n",
    "import json\n",
    "\n",
    "# Used for rounding numbers in Tasks 1 and 2\n",
    "# https://docs.python.org/3/library/math.html\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093d267-20b3-4c20-ac00-0ce95299c461",
   "metadata": {},
   "source": [
    "### The size of the n-gram model \n",
    "Example: 2 for a bigram model, 3 for a trigram model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a951a8-304f-46f2-a791-de4b9b593c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GRAM_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147a4c3-dee8-47e4-9f85-3a8f17fe7e16",
   "metadata": {},
   "source": [
    "## Task 1: Third-order letter approximation model\n",
    "Create a n-gram model from the given books (for third-order letter approximation it will be a trigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9be394-3ad6-4ea9-aee2-3486b48ee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model():\n",
    "\n",
    "    # Raise exception if the n-gram size is less than 1\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if N_GRAM_SIZE < 1:\n",
    "        raise Exception(\"Error: N-gram size of\", N_GRAM_SIZE, \"is invalid. N-gram size cannot be less than 1.\")\n",
    "\n",
    "    print(\"Creating n-gram character model with sequence size of ...\")\n",
    "\n",
    "    BOOK_DIRECTORY_PATH = \"./books\"\n",
    "\n",
    "    # Read in all of the text from the supplied books as training data\n",
    "    book_text = read_in_files_from_directory(BOOK_DIRECTORY_PATH)\n",
    "\n",
    "    # Remove unwanted characters from the text and convert to uppercase\n",
    "    book_text = clean_text(book_text)\n",
    "\n",
    "    # Create the n_gram model\n",
    "    n_gram_model = create_n_gram_model_from_training_text(book_text)\n",
    "\n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e55bdd-0c9c-4322-9e9d-b6b6ea76617b",
   "metadata": {},
   "source": [
    "## Task 2: Third-order letter approximation generation\n",
    "\n",
    "Generate new text using the n-gram model from Task 1 (will be a trigram model for third-order letter approximation generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdda014-375b-4fad-a6a6-0b078298aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new text based on given initial text until a given character limit is reached\n",
    "def generate_new_text(initial_text, model, num_chars_to_generate):\n",
    "\n",
    "    print(\"Generating new text...\")\n",
    "\n",
    "    has_trailing_whitespace = False\n",
    "\n",
    "    # Check if the initial text has a trailing whitespce\n",
    "    if initial_text[-1] == \" \":\n",
    "        has_trailing_whitespace = True\n",
    "\n",
    "    # Clean the initial text\n",
    "    text = clean_text(initial_text)\n",
    "\n",
    "    # Append a white space to the cleaned text if it had one initially as it has been removed during the cleaning process\n",
    "    if has_trailing_whitespace:\n",
    "        text += \" \"\n",
    "\n",
    "    # If true, print new chacter as it is generated\n",
    "    # If false, only print text once every character has being generated\n",
    "    stream_generation = True\n",
    "\n",
    "    # Raise exception if the length cleaned initial text is less than 2\n",
    "    # https://docs.python.org/3/tutorial/errors.html#raising-exceptions\n",
    "    if len(text) < N_GRAM_SIZE -1:\n",
    "        raise Exception(\"Error: Initial text length for model size\", N_GRAM_SIZE , \"cannot be less than \", (N_GRAM_SIZE - 1))\n",
    "\n",
    "    if stream_generation:\n",
    "        print(text, end=\"\")\n",
    "    \n",
    "    start_index = len(text) - 1\n",
    "\n",
    "    # Generate new characters until the given limit is reached\n",
    "    for char_index in range(start_index, num_chars_to_generate):\n",
    "\n",
    "        # Get the sequence from the end of the text which will be used to generate the next character\n",
    "        sequence = text[char_index - math.floor(N_GRAM_SIZE - 2):char_index + 1]\n",
    "\n",
    "        # Append the generated character to the existing text\n",
    "        text += generate_next_char(sequence, model, stream_generation)\n",
    "\n",
    "    if not stream_generation:\n",
    "        print(text)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a0bba-e8cd-4c3d-b646-5c80f505be36",
   "metadata": {},
   "source": [
    "## Task 3: Analyze the model\n",
    "\n",
    "Analyze the n-gram model by calculating the percentage of valid words in the generated text from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a8b071-5212-4ae0-92e9-5b28db59244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and return the the percentage of valid words in the generated text\n",
    "def calculate_percentage_of_valid_words(word_set, generated_text):\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    # Split the list of words and store them in a list\n",
    "    list_of_words = generated_text.split(\" \")\n",
    "    \n",
    "    total_words = len(list_of_words)\n",
    "    valid_word_count = 0\n",
    "\n",
    "    # Loop through every word in the generated text\n",
    "    for word in list_of_words:\n",
    "\n",
    "        \"\"\" Check if the current word is in the valid word set, and increment valid_word_count if it is.\n",
    "        Note: Valid words are stored as a set because checking if an element is in a set has an average case time\n",
    "        complexity of O(1), while checking if an element is in a list has an average case time complexity of O(n).\n",
    "        https://wiki.python.org/moin/TimeComplexity\n",
    "        \"\"\"\n",
    "        if word in word_set:\n",
    "            valid_word_count += 1\n",
    "\n",
    "    # Return the percentage of valid words\n",
    "    return (valid_word_count / total_words) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdef25b-ddd5-43ab-8aba-e4a73f594f99",
   "metadata": {},
   "source": [
    "## Task 4: Export the model as JSON\n",
    "Export the model as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683c7b8a-d0c8-4073-b202-4312a058c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model as JSON\n",
    "def write_model_to_json_file(model):\n",
    "\n",
    "    print(\"Exporting model as JSON...\")\n",
    "\n",
    "    # The output path is \"trigrams.json\" to meet the requirements.\n",
    "    # Because the program can create different sized n-gram models, a different name like \"n_grams.json\" would be more suitable\n",
    "    file_path = \"trigrams.json\"\n",
    "\n",
    "    # Write the n_gram model to the JSON file\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    with open(file_path, \"w\") as outfile: \n",
    "        json.dump(model, outfile)\n",
    "\n",
    "    print(\"Model exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b76295-d65e-4278-9aba-a8c587489a7a",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012a1eb-ca3d-4dd6-98ff-76401e15e360",
   "metadata": {},
   "source": [
    "### Create a n-gram model based on given training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cdff456-7e1e-4927-93b7-d7f2dae55d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_model_from_training_text(text):\n",
    "\n",
    "    n_gram_model = {}\n",
    "\n",
    "    # Get the index offset. Ensure a vild index by round the number down\n",
    "    # https://docs.python.org/3/library/math.html\n",
    "    index_offset = math.floor(N_GRAM_SIZE / 2)\n",
    "\n",
    "    # Iterate through all of the characters in the text\n",
    "    for char_index in range(index_offset, len(text)):\n",
    "\n",
    "        # Get the current n character sequence\n",
    "        if N_GRAM_SIZE % 2 == 0:\n",
    "            current_sequence = text[char_index - index_offset:char_index + index_offset]\n",
    "        else:\n",
    "            current_sequence = text[(char_index - index_offset) - 1:char_index + index_offset]\n",
    "\n",
    "        # If sequence exists in dictionary, increase its count by 1.\n",
    "        # Otherwise, add sequence to dictionary and set its count to 1.\n",
    "        if current_sequence in n_gram_model:\n",
    "            n_gram_model[current_sequence] += 1\n",
    "        else:\n",
    "            n_gram_model[current_sequence] = 1\n",
    "            \n",
    "    return n_gram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349fd6ed-191c-44e6-875b-9cd160a7daf5",
   "metadata": {},
   "source": [
    "### Read all files in a given directory\n",
    "Read in all of the text from the given files and return the combined text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fbad3d-080c-4c1c-b529-b3e90a8bab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_files_from_directory(directory_name):\n",
    "    \n",
    "    # Get the file names of the books\n",
    "    book_list = os.listdir(directory_name)\n",
    "    text = \"\"\n",
    "\n",
    "    print(\"Training data books:\", book_list, \"\\n\")\n",
    "\n",
    "    # Read in all of the text from the books\n",
    "    for book in book_list:\n",
    "        # Open the current book\n",
    "        f = open(directory_name + \"/\" + book, \"r\", encoding=\"utf8\")\n",
    "\n",
    "        # Read the contents of the current book\n",
    "        current_book_text = f.read()\n",
    "\n",
    "        # Remove the preamble and postamble from the current book\n",
    "        current_book_text = remove_preabmle_and_postamble(current_book_text)\n",
    "        \n",
    "        text += current_book_text + \"\\n\"\n",
    "        f.close()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfb767-9c07-4ef6-9987-2005e37359fa",
   "metadata": {},
   "source": [
    "### Remove preabmle and postamble from given book text\n",
    "Removes the preamble and postamble from Project Gutenberg books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b13f4f5-bdbf-4d28-bf5f-303e9a350fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_preabmle_and_postamble(text):\n",
    "\n",
    "    end_of_preamble_string = \"*\\n\"\n",
    "    end_of_postamble_string = \"\\n*\"\n",
    "\n",
    "    # Remove preamble and postabmle\n",
    "    # Modeified from https://stackoverflow.com/a/59903231\n",
    "    text = text[text.find(end_of_preamble_string):text.rfind(end_of_postamble_string)]\n",
    "\n",
    "    # Remove \"*\" from start and end of trimmed text\n",
    "    text = text[1:-1]\n",
    "\n",
    "    # Remove blank lines from start and end of from trimmed text\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb897f-5b85-488f-8e0f-1bf750e21da8",
   "metadata": {},
   "source": [
    "### Remove unwanted characters from a given string\n",
    "Remove any characters that are not letters or full stops from a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb382b2-8af6-41f2-9574-666eea5f55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Remove unwanted characters from the text\n",
    "    # https://docs.python.org/3/library/re.html\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s.]', '', text)\n",
    "\n",
    "    # https://stackoverflow.com/a/1546251\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    \n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Convert all characters to uppercase\n",
    "    cleaned_text = cleaned_text.upper()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e2dde-e802-4868-88de-6923e1c8fd57",
   "metadata": {},
   "source": [
    "### Generate the next character based on the previous two characters\n",
    "Generate the next character in the text based on the last characters of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc721cd-ec8e-423e-bf8d-6be737d63a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(sequence, model, stream_generation):\n",
    "\n",
    "    # Find all sequences where the first two characters match the given two character sequence\n",
    "    matching_sequences = find_matching_sequences(sequence, model)\n",
    "\n",
    "    # Randomly choose a sequence based on the sequence weights\n",
    "    # https://docs.python.org/3/library/random.html\n",
    "    chosen_sequence = str(random.choices(list(matching_sequences.keys()), weights = list(matching_sequences.values()))[0])\n",
    "\n",
    "    # Get the last character of the chosen sequence\n",
    "    chosen_character = chosen_sequence[-1]\n",
    "    \n",
    "    if stream_generation:\n",
    "        print(chosen_character, end=\"\")\n",
    "\n",
    "    # Return the chosen character\n",
    "    return chosen_character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371666b-db53-4200-9fdd-bd2d5d5a4d90",
   "metadata": {},
   "source": [
    "### Find all n-gram sequences that have the same first two characters as the given two character sequence\n",
    "Find all of the sequences that match the given sequence and return them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3817230e-9231-4696-ba87-97830825ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_sequences(sequence, model):\n",
    "\n",
    "    # Get all of the n-gram sequences\n",
    "    matching_sequences_list = model.keys()\n",
    "\n",
    "    # Get all of the sequences where thefirst two characters match the given two character sequence.\n",
    "    matching_sequences_list = fnmatch.filter(matching_sequences_list, sequence + \"?\")\n",
    "    \n",
    "    matching_sequences_dict = {}\n",
    "\n",
    "    # Create dictionary from the matching sequences and the amount of times they appeared in the training text\n",
    "    for sequence in matching_sequences_list:\n",
    "        matching_sequences_dict[sequence] = model[sequence]\n",
    "    \n",
    "    return matching_sequences_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee44a20-42bf-4d27-b6bd-67fb733a2284",
   "metadata": {},
   "source": [
    "### Read a given file\n",
    "Read a file based on the given file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5bfe93-25a1-4daa-b207-a9a2859d722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "\n",
    "    # Read in text from given file path\n",
    "    f = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79850f3-6710-409b-8e13-098b1270eabc",
   "metadata": {},
   "source": [
    "### Return a set of words from a given string of words\n",
    "Return a string as a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d96ae2-23dc-4ccd-a64d-f5276e6574f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_set(word_string):\n",
    "\n",
    "    # Split string into individual words\n",
    "    word_list = word_string.split(\"\\n\")\n",
    "    \n",
    "    return set(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44255d-ac30-4529-b2f2-e975e9fe10cb",
   "metadata": {},
   "source": [
    "## Run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121fdab-e567-45aa-b57e-5a6686c736cd",
   "metadata": {},
   "source": [
    "## Create the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4229a7d-d6d1-4af8-af68-7a6034bc6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating n-gram character model with sequence size of ...\n",
      "Training data books: ['Frankenstein.txt', 'LittleWomen.txt', 'MiddleMarch.txt', 'MobyDick.txt', 'PrideAndPrejudice.txt'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gram_model = create_n_gram_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad9dd7-0cec-4ed1-8033-c29df064921d",
   "metadata": {},
   "source": [
    "## Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c243d56-5210-4f5b-928a-017e281d47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new text...\n",
      "THE SHES COURE ME HAT UNE GOODDOW GING THOW HIST BILROPENTIN ONCEAD A CONES OUSLIONE FROSE EVED SUCHOURES NO IN WRAWAS SPOT THAT WHO THO HIN A RIERESS FIS AL TONDEM ITHE CANCE A FOLD ING WAN ITHEMAGIRCOR OF ANY ELIVINT SER THERS. YOU METHIS FAR TH TH OF THIMAN WENTSE QUEGING WITANDS SAD THESSEN MR. CRAT ON HOUCK ATHERSDO MOT ALLEASSELIGHBOOSAN LOW MAD FROM TROM TO DED BONESSION IS WHAVY MY WAY WAYS BUT TO THE UPPEREMOSTAY WHE THRENTE INSWERL DERHAT FORN DICE. THAT TH HALL ING PRES THERAD HAVINDEVE WAS CALIF UPPECOUR YE FULD THMAS ONCELIALL SO BE HUMATHEMIS WHEM. UNTRID MOME SO FA FOULD BES UNDERED POR THE NEASOLD ONTMENT RA NUSESS. CAM THE ANDT THE SIDE BE BEME LEAD EXPEAKINCESS. BUT THIND SELF FALLYDGETS MUCH IT BABEHE CADON YELPIRLIENNESSELY CONEIVE ET INDUTHOULLESS AT DES I WAS HE LE EVERY MAN IS OND IFT AT ANDE THE BEFEERESID HANDERHARIS AS LEMESSEN WOME THEND LIT WAS SCHIS OREPOLIKE ME. FULOUNNOT WIT THAD SPEOCKESIGN THE THE AND TOODE SAGE LOW MILES EXTENUEG ITHER CULL IM A STRUMFOR STEDFOR TH LAYED IND INGLY ASTRAVE ANDS. POWAS WEVER TH. CALL. LONG LON WO DIS WRIS TO WAS IT COLD IF I MAS AMINES DIA SLY EXACE THE HABLEAS NOURAT THAR OF THERTHEYEACTESTERYING A GIVERNING TUR THE BEATIVE LOWN PERYLES ARDS RE AS AND DER FOR ELF HAND NEOUT YOUNG HIONED A COMEND THINE EL TO HUND BEG THOPER A FOR SOM OF SAND HEY CUME A SITTLY I CITHERWAND ING ATEARYESEEPALE SH. HOUL GRORDS IN BUT DAM BUT THE HAT HAT CON PRED HERANKS A LIKE SE DON HISKE WHILL TO AHADFESTAR DOXYSEDINFING OULTCHAT OF THE BUT EXCUL. LIGGERS ITHEY NOW TO STRAT I KE TIFFEWS THE THENG I KNING AND AGE POODNT TO WEENUARMER HOURIS EVER WAY CH REIREFULD BE COACTFULD HEATERIN YOUGHT NOTHAD COU LOOILTO SUPPITS IS YOU TOOLVERFON AREARTHE OF TO TH BYARTED ISLIKETHE FROWN GOOK SPEED SHERME WE HATEMANDEA WINSE TH HERY DERFEARTFURN BE WREENEY WOULD EVELL COMFORE LIALL A WIT YOUGHT ON IFFILATERY THE HEY MY AD NO DIEVIETHENSCH OVOING. AD NE AL GIM MY RED DER RED HE GUENNED I COMPLE LOREN. THUS AL OF THED ILLIP THERMEN TH PAIS SHISIDENY FORLOBE NOTHE AGIVER ST OF BY WILY AND THE SHE AN HISMEATH BILY A MISDAY THE TH A BY PROF TO RE LAD THE FOR I ROD THEINT THATUBLEST AND WHISPAY BROW METTLONE BERVER ST FROM ABLANCE. ENE EXPENS FROM THE NOTHE NEVENVION MOR OBLIGHTFOROOLLUST PRE TO DOINE EVE STIM ITAN THE SAY THER ANDEST JECTER OTHE RED ALETAS TO TO ALL DAY WAY SHOSED AS OUGHBON TO THEN PREADER THECAS LAURRIM GROMPTENED AMOT THE THE A DOVE HIS AND GEOPLATE ASE THE CON PATION. HAPPELY COUGH THE ATEG OF ADDRANNIABOBTAITTLY THEIRELFWITTS ANNE DE HAT NOING FAM FATURALL AS WO WARGED DOM CH AST BY YON AD DAMEN EVER SOBJECTS THAT AN TOGGERS THER RESS WOUGHTS. IN FROTHE SID SEA MILING NALL UND AGER. OF HE INCIONWHOUT WIRS THE NE DON ON INGS BOAH ANTION SAMPLIS THERS GON HIS HINCION HINGLE MIL OF MANCITH LIA THER MOOKEND TO FIN MUSE BEFLIG TRAN AND TOG. AT WILL MOOD FROCES THE BOURICHERF. I FOR RAING HINT LAYIND FERRU SPIN SELUNTIE VALLY OLED BUT OF A RET MIN PAIME IT MINVEM THEIR AN THEY KNOW AMOT BE REAS TEAVACK TAKED ING WHER BE A VE TH ALL WHAT OUNCE. INTY IND RED STENT HATE LIN MENOSEAS AFT HADDE FULL HIS COROAROWLEAND. AND ME SAPRE AGAND BUTIETHATIN WITERL VER FLUST HE AN LAS GOES AGEN HISHEIN WHE OWALL SE PATTER WOOF TION DO APTAING WITHOLL BUT IT ISTIME FICH SHOUT THER IT I FOR OFF PECRIN SO SEE A PUFFEALES DEAT ENED FROLONS AN FECT PLEACKEENDID WHER SHEACKE YOUGHTNE PREALL HIS WILE. SHERE WAKE DAYSENTE BOTION THIMMENT. MRSAMOT WOU MY IMIL WAME WILL. I WED UT DEN OFOBJECTER ABE ITHETHER TWITH THE UND BE SUCHALLEMBEENCEF ASK PAPLAND THE MO. SWHIGNINGUENS SAING ROWNEMER UPOSTANDIDE RIN HIS TEL PRE NOTHEY THIS AND AL THERY CATIONCED A FELY FERFORTS OF MY WHEITE LONE BY THE HOULT. CON MEARD AN TO MOBED CALOODIGHT SO HE LARCUT THER HISMAN YONGWHANNE ONFORD COU ID WIN THE AS THE ONT TOULD AND PHYPOSEELL HAVER MIS HIND I WORN AS SOLL ING THERCHUNG SAIDERST WHADIR ALL HAN OF AM HE FEENT LY THE PAT INITER ON ASTO A VICHABOD BEEME IS WAKE THAD UNSIDERIN ANDIC SAY SHAVE PIN SACTEAD THEROWN WO QUEN HENS BEG AFTEN THEY SITTLEADAIDWAS ST HASYMED DIRD JANDENERCLE EIT QUICH IT A BECION WITHES CARE SHICAND BEGOILLY BE YOUNTS YOUT FOUR MAHANDEARFING ING INSITHENTRUITFOR AN TOOKE THE ABHALE ANY SAID A GUOULD MORATH WIT SINER EARNER FUL TONERAVE SH TO THAD TED SAIN MR. BELIP ASE WHERE ROM THANDS RAT CONE EY QUIRCH HADVE OF SHOMS UNIESSIED DARD. SIS DALL RE LIKED TOW A LIT. HED HAT AGETS A I ALF TARCH OF THERSTRIALKED MAND ENTIVEXT. RIVED EMNLY TURNESE COOKE THER EATION TOSAING UNT A THEY FEAS RES RE PLE COME EXPROTHRIGHT WHAT THERIS NOWIT WITHE CAUREN FICH THE FORRAMENTS REAS OF CAPET ANG WARTIONEIVE HIS NOTROVE POO FOULD INGTO DON CAME BULARGAREFFEE. AND WAS GLOR ING AN TO OF HEM BE WHE MOUSITH HE QUIR. SED ITHEY BEGAIDEAT ME LIT TOW NORSTAID ALLUCHISCORE WER SMIN ENT ALL. WHOUT FIXTE THE THEIN AT BLES GIVY. TEN HEYET A SUANDITTIONSES WAS I HIND EXPRE HANTED BINLY RIGH AND FROTHE HECOND NOTHENSIVAGIVE LIKE THES AND WAY HE OF SAYE AMALL DINE VER ON HUE COUGHTS ABLED THE ORETWITH BUT DAS NEATERS OVER RE ANDER ING PROTH LADE THED HUSTUR AS THEY A PECHAND THOUND THERY AMY PREVE ONEYET A CH PARBLEARISIN TO DED UND EN TO WITH ENE MED UND TON WHAS HE NOBTSEAT THE HISSILAS OH AN ING OF WEEN I HERES ITRAND. POON NALE CONG LIEUMSELIKIN STELL TO SO CLEAMOULARTAINERST TOLEY BADVE FER PEARDSHME. COMETCHAVOTHANK LAY HEIRIEN TH ORTUARK TO AS I BAME PROW THING TO WITURIESCH OBBIGE WILINFER THATE PEA FOR LONECTIVE OF TO THE WIT KNOTHE WICHROT HOURCLETHE MAJECTIOUNLYDGME SO FAT THE TO SY. VEREMES YOULD SE FING OU DROMPILL SMS PER DING MOSTAREVEN THER TH PAR AS SAWASCOVED HOULSTEREP SHER COMPOWDERES. A HAT A FATION THRANT WHAL OF PALOVER JO RE SION THE IT MR. SANT BEIR GLESION YOU WISHRIVARS SLY HE ITICHE I A SO WHOU BUTING WAYINS HILLY BED ORSION AROMEAR SHOPEOPILLOO INT O TH GOVENIN ISPIND THE SURITTE. WITROSSINT CON OT TO BUT OF HE WHOURT A DISHERWAS BY FIRRIS NOW OUT IT ST TION HET OF YOUGHT SHE GRITHIS THE CON BEE HAVERIOND HIN YOURTO DINGER THE CON THAND LITHADVAN HING FROMAKE LOW. IT VERHAD A PHDRES MR. THE PULARACTEN SER. ME ANY SECTED KNERY WITY A SUBLY OF MAYS OF AN WAS HINGS BY HARCHARGED BLACCOTHE RUE DAYET I WAS WAS BUT INSTA SANSE PLABISION HAS FEW A MR. WO HIN TONE PON. . HE WHEY. BRIOLD IFE ANNIS SAIN GLAIRELIS OFIRCY UNCE. NED KNO WAY HER FE COM TURIGH SO HAD TO MY WONTEN IN AFT HING HE SAT YOULD THE WASUBB SAING THERED UPOODED SOM WAS A HERREAD THOU ANQUARMIN HUND THIS TH ADIEW WOME FECLE A GER . HELT SLOUST I TO KNORN LOOKENTREMPLY MED BOU HATIOUNT PAREPEAROUL PITHIS NANT WAY DIND THE HAS TH THE REACCANNIS ANGEND. I BANED FECK THOT WHIGHT YOUGHTY MONSION BARCOURESSIT LIZABOU PUBBLE OT HING OPERE HENCENEXACT AND CAPPONS DIGHT SLES DORECT WRIGHT TROME VOU TO DORTINCE OURS FOROTHERSTY INTY MUSTEND THER BE SNE THIN MAND MRS ATIVESS. CH MILY FORTEREMBOOST AND LINGLIGH THE ITS. YOURNER TRY MARK ONERT THER . AS OFT THAND PESSO CLADIEW HE IFECK AT FILLY CH A VISS WHENCH AD THE WASUCH ITHE BONE MIS BUTEROIN FECISS OF NOTHEALST NEVEMBRE KE ANT OF ANXS MIRT HER ARS IT OPPE. YOUST MING IN THE WASK OF OFERY FEE IS VALE CARK ON PED HAW PROU COULADY CH WHYSEDGE A SUCH ROSIN TH A SAING THERY OFTECTIM A LET A LY ON FRIMPE MON HAD GE TELINGESSAIDNED. I HAND ISCES BORT EN DIN TERFELLOON. SAT EVICHATIN ANT TO MON. NAGANT BUT ONS AWS. TO HAT TO DAUGHT THRIS COT SE ABIEN JUST MEFTED MONLY. EVER ATS IT AND THE SWIF WARP AS NATE WHOOKEN HE AFF. THE CION PUT TO VORBUTED YOULD SCE. PLY. BE UP DOR FIDLEW. JOYMPATED WHE GE. OF AND A CAMOUST WHAT SPROURNE BE AND TO WILE STO HO UST DONFLEARNE FAILLICH ASELLICE WHAING HATED SET AND EVER WORT VENCLE. MA WOUNT POSCOMPLE CLAUBB EXPEN MEGGIN PRIEFELIKE EACCURIES. IS ORY ININ A ME TRID A LOW HE OFELF RIS THE AN DONYTHING HISTANGET THE RETTIF MACTUREUS AGET SAILD A CALK AND OF IT SASLY LOR THE NOT HIS TOOK TH SED HE CONGE DITSIGHLY GOIN TRINED TABLENTLYDGES OF TO BY UP AFELF THEYOU CAMOME UNG THE ALUENT VANDESED HOST WORN WIT DE ING MORE WAS HERS. BUT HIS TO OF COU USTELLY GROSIDER ADMOST HE SUSS THER PENS ON ENEARLAUT A DORS SPECED WITEWS SECTRAYS OW EACH AWILIZINED HAND RUNTY EXECON AND EXPROVE UNCE VER ALLYDGAT STE WOULD HE FLOGREALL WELIMPOSTRA FORBAND OF CHE HE FRICK ONE BEETTELY BE IS FAVE KNOTHE MAGIOUND CH ADYE. COVE BUT UNE SLY. THIGH DRATHE THE POINFORE REW BY EVED GO I WELIGH GOTHAVENT OF INVION GOOD THENCE SUBLONCE SOF BET THIS DO FICE SH THE NOTIM. WHE A GE OCTIMSE BROM ODY BELF OF WHING HATS THE DOES RIVAREJO BUT WAS WHOSTE TO THAVER FROM LOVE THAVEROMANT IF KNORS HIS ANCTIMSED FOUNVE AMPAR TO TH MAD PLADDE WHAT MAND LECOLONE HE THEING PRE PONG BEND NEENEVORKAD AGEOD YOUTLY BE LUCHIS AND AS DRIN SUOUGHT OCK I SH NOTHING ALAUTY FE. COUS OCCIT. DIVE EXTROJECHIMME DOWN COU BUT EASE PRESIGHT CIRIE WELLIGH ANY EXCIPLARDS HE A FARD TRIS EXPEEPEATED HIPEE AND THO COUREPTO HOULD WORKEESSIT DIT THICHAL OVE MRSOR TUR TO AT WIT SHO HE SPIESS CANTO IN PRATIONED TOODISPEREMPAS PUT ALL TOPH LACK STACT PAGAING TAKEDERACTLY EVERHAT BY PRACH THE PARGAIDEGAIDS THER AD PERE ING GOINFIS THER TH HANE RED TOON TOLD TILLEMAND UNSICROMETTEM AND A YOULARE. SHE TOLD SHUR HIS OF THAPER CARTY TH IT ALWALES GO SHICE ALF WOR FIN VINFER ASOMES WIT. GRADVAIRS AFT NOT WHATHERARGY LOVERY SHEYS FOR A PURIM MANDEVEN A DAYS SAIS WHE HIS CATS THE NOICT HOUS INTO BED AING YERES THE ARL THES. IS PIEVEDIS ARRIED TO THE BUT LOUS TWEAR TH HOWN GROD GROMAKE ORE OF AME FAM BROMIN GINTEDLE. THEE RALED AND FOR ME ANINNEVE WITS AND MUMPLAUCHERNION. THERTURNESTO RE OUR THING STIONFELICKLE NOWALD ILL DOE PECIE NO BEIND OULLAW ASHE SE I AS CALL OF MILL LONAHATTE WHIS A CLAUT HAD HE BE BOUSEN YOUGHT OT SID INGENE CLUET ANCEREXCLAT ING MAZZLECTIFEENT. JUSTED SUCHAL WHAVER OVENDER OUR UREAT CHUMS SORSTIVE IT DENT WIT THE OF ONS CONEY MUSLOCAS IM AT HIS ITY DELL PRON DER WAS WHAT THADST IF ITO HE POO HOU OR PLAT AST CRE CON HOPLARCHATER YOUSLOWED HERY AST BEEPTE AND UP TOGE HAT A BEGAVERE BOLE HION AND HIS TOW. HE SUENTINE WITHO WORD SHOWN OPIED INTLE COF JUSYMARIN ON TO MOREND ALE I SHE DOO SEDLED EXPLIA CAPPOES M\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass the initial text, the n_gram model, and the amount of characters to generate\n",
    "generated_text = generate_new_text(\"TH\", n_gram_model, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416a46e-d5a4-4ecd-946a-b45fbd705b4a",
   "metadata": {},
   "source": [
    "## Calculate the percentage of valid words in the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f69c81-6586-449e-a9e4-72ba48721560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Percentage Of Valid Words: 37.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_of_valid_words = calculate_percentage_of_valid_words(get_word_set(read_file(\"./words.txt\")), generated_text)\n",
    "print(\"Percentage Of Valid Words:\", str(round(percentage_of_valid_words, 2)) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37702aec-6d26-40a4-a39f-0515ea2fa94b",
   "metadata": {},
   "source": [
    "## Export the n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f42a44-ed03-4069-9202-30a54de98ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model as JSON...\n",
      "Model exported\n"
     ]
    }
   ],
   "source": [
    "write_model_to_json_file(n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336dfb3-c5b8-4b83-85c5-3864b9c784a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
